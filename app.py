from fastapi import FastAPI, Request, UploadFile, File, Form, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.templating import Jinja2Templates
from fastapi.exceptions import RequestValidationError
from typing import List

# ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™ã‚’è§£é™¤ã™ã‚‹ãŸã‚ã®è¨­å®š
import uvicorn.config
import sys

# FastAPIã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™ã‚’å¤§å¹…ã«ç·©å’Œ
if hasattr(uvicorn.config, 'MAX_FORM_FILES'):
    uvicorn.config.MAX_FORM_FILES = 10000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1000ã‹ã‚‰10000ã«å¢—åŠ 
    
# Starlette MultiPartParserã®åˆ¶é™ã‚’ç›´æ¥ç„¡åŠ¹åŒ–
print("ğŸ”§ Starletteã®MultiPartParseråˆ¶é™ã‚’ç›´æ¥è§£é™¤ä¸­...")

try:
    import starlette.formparsers as fp
    
    # MultiPartParser.__init__ã‚’ãƒ‘ãƒƒãƒã—ã¦åˆ¶é™ã‚’ç„¡åŠ¹åŒ–
    original_init = fp.MultiPartParser.__init__
    
    def unlimited_multipart_init(self, headers, stream, *, max_files=50000, max_fields=50000, max_part_size=200*1024*1024):
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å¤§å¹…ã«ç·©å’Œ
        print(f"ğŸ”§ MultiPartParseråˆæœŸåŒ–: max_files={max_files}, max_fields={max_fields}")
        return original_init(self, headers, stream, max_files=max_files, max_fields=max_fields, max_part_size=max_part_size)
    
    # ãƒ‘ãƒƒãƒã‚’é©ç”¨
    fp.MultiPartParser.__init__ = unlimited_multipart_init
    print("âœ… MultiPartParser.__init__ã‚’åˆ¶é™è§£é™¤ç‰ˆã«ç½®æ›")
    
    # on_headers_finishedã‚‚ãƒ‘ãƒƒãƒï¼ˆå¿µã®ãŸã‚ï¼‰
    original_on_headers = fp.MultiPartParser.on_headers_finished
    
    def unlimited_on_headers(self):
        # max_filesãƒã‚§ãƒƒã‚¯ã‚’äº‹å‰ã«ç·©å’Œ
        if hasattr(self, 'max_files') and self.max_files < 50000:
            self.max_files = 50000
            print(f"ğŸ”§ å®Ÿè¡Œæ™‚max_filesåˆ¶é™ã‚’50000ã«æ‹¡å¼µ")
        return original_on_headers(self)
    
    fp.MultiPartParser.on_headers_finished = unlimited_on_headers
    print("âœ… MultiPartParser.on_headers_finishedã‚‚åˆ¶é™è§£é™¤ç‰ˆã«ç½®æ›")
        
except ImportError as e:
    print(f"âŒ starlette.formparsers ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
except Exception as e:
    print(f"âŒ MultiPartParserãƒ‘ãƒƒãƒã‚¨ãƒ©ãƒ¼: {e}")

# Requestã‚¯ãƒ©ã‚¹ã®formãƒ¡ã‚½ãƒƒãƒ‰ã‚‚ãƒ‘ãƒƒãƒ
try:
    import starlette.requests as req
    original_form = req.Request.form
    
    async def unlimited_form(self):
        print("ğŸ“ åˆ¶é™è§£é™¤ãƒ•ã‚©ãƒ¼ãƒ è§£æé–‹å§‹")
        try:
            # ä¸€æ™‚çš„ã«MultiPartParserã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å¤‰æ›´
            import starlette.formparsers as fp
            
            # å…ƒã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‚’ä¸€æ™‚ä¿å­˜
            original_constructor = fp.MultiPartParser.__init__
            
            def temp_constructor(parser_self, headers, stream, *, max_files=50000, max_fields=50000, max_part_size=200*1024*1024):
                print(f"ğŸš€ ä¸€æ™‚çš„åˆ¶é™è§£é™¤: max_files={max_files}")
                return original_constructor(parser_self, headers, stream, max_files=max_files, max_fields=max_fields, max_part_size=max_part_size)
            
            # ä¸€æ™‚çš„ã«ãƒ‘ãƒƒãƒé©ç”¨
            fp.MultiPartParser.__init__ = temp_constructor
            
            # å…ƒã®form()ã‚’å®Ÿè¡Œ
            result = await original_form(self)
            
            # ãƒ‘ãƒƒãƒã‚’å…ƒã«æˆ»ã™
            fp.MultiPartParser.__init__ = original_constructor
            
            return result
            
        except Exception as e:
            print(f"âŒ åˆ¶é™è§£é™¤ãƒ•ã‚©ãƒ¼ãƒ è§£æã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å…ƒã®ãƒ¡ã‚½ãƒƒãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await original_form(self)
    
    # ãƒ‘ãƒƒãƒã‚’é©ç”¨
    req.Request.form = unlimited_form
    print("âœ… Request.formãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆ¶é™è§£é™¤ç‰ˆã«ç½®æ›")
    
except Exception as e:
    print(f"âŒ Request.formãƒ‘ãƒƒãƒã‚¨ãƒ©ãƒ¼: {e}")

print("ğŸ¯ Starletteã®MultiPartParseråˆ¶é™è§£é™¤å®Œäº†")
import numpy as np
from deepface import DeepFace
import onnxruntime
from PIL import Image
import os
import tempfile
import urllib.request
import uuid
import shutil
from fastapi.staticfiles import StaticFiles
import cv2
import asyncio
import json
# Removed unused imports: ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing
import gc
import time
# Removed unused import: partial

# Try to import psutil, fall back to basic monitoring if not available
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("è­¦å‘Š: psutil ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬çš„ãªãƒ¡ãƒ¢ãƒªç›£è¦–ã®ã¿ä½¿ç”¨ã—ã¾ã™ã€‚")

# AVIFå½¢å¼ã‚µãƒãƒ¼ãƒˆã®ãŸã‚ã®ãƒ—ãƒ©ã‚°ã‚¤ãƒ³
try:
    import pillow_avif
except ImportError:
    print("è­¦å‘Š: pillow-avif-plugin ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AVIFå½¢å¼ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¾ã›ã‚“ã€‚")

# Buffalo_lç”¨ã®ã‚«ã‚¹ã‚¿ãƒ DeepFaceãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹
class Buffalo_l_Model:
    def __init__(self, session, model_info):
        self.model_name = "Buffalo_l"
        self.input_shape = (112, 112, 3)
        self.output_shape = 512
        self.session = session
        self.model_info = model_info
    
    def predict(self, img_array):
        """DeepFaceäº’æ›ã®äºˆæ¸¬é–¢æ•°"""
        try:
            # å…¥åŠ›ã‚’æ­£è¦åŒ– (DeepFaceã¯0-255, Buffalo_lã¯-1~1)
            if img_array.max() > 1.0:
                img_array = (img_array - 127.5) / 128.0
            
            # CHWå½¢å¼ã«å¤‰æ›
            if len(img_array.shape) == 4:  # ãƒãƒƒãƒå‡¦ç†
                img_array = np.transpose(img_array, (0, 3, 1, 2))
            else:  # å˜ä¸€ç”»åƒ
                img_array = np.transpose(img_array, (2, 0, 1))
                img_array = np.expand_dims(img_array, axis=0)
            
            # æ¨è«–å®Ÿè¡Œ
            input_name = self.model_info["input_name"]
            embedding = self.session.run(None, {input_name: img_array.astype(np.float32)})[0]
            
            # æ­£è¦åŒ–
            embedding = embedding / np.linalg.norm(embedding, axis=1, keepdims=True)
            
            return embedding
            
        except Exception as e:
            print(f"Buffalo_läºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            raise e

# DeepFaceã«Buffalo_lãƒ¢ãƒ‡ãƒ«ã‚’ç™»éŒ²ã™ã‚‹é–¢æ•°
def register_buffalo_l_to_deepface(session, model_info):
    """Buffalo_lã‚’DeepFaceã®ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ç™»éŒ²"""
    try:
        # Buffalo_lã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        buffalo_l_instance = Buffalo_l_Model(session, model_info)
        print("Buffalo_lã‚’DeepFaceå½¢å¼ã§åˆæœŸåŒ–ã—ã¾ã—ãŸ")
        return buffalo_l_instance
        
    except Exception as e:
        print(f"Buffalo_lç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")
        return None

from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(
    title="ArcFace Face Recognition API",
    description="é«˜åº¦æœ€é©åŒ–ã•ã‚ŒãŸé¡”èªè¨¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ",
    version="2.0.0"
)

# å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãŸã‚ã®ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢è¨­å®š
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response

class LargeFileMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆã®ãƒ­ã‚°å‡ºåŠ›
        if request.url.path == "/compare_folder":
            content_length = request.headers.get("content-length")
            if content_length:
                size_mb = int(content_length) / (1024 * 1024)
                if size_mb > 100:  # 100MBä»¥ä¸Š
                    print(f"ğŸ”¥ å¤§å®¹é‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ¤œå‡º: {size_mb:.1f}MB")
        
        try:
            response = await call_next(request)
            return response
        except Exception as e:
            print(f"âŒ ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã‚¨ãƒ©ãƒ¼: {e}")
            from fastapi import HTTPException
            raise HTTPException(status_code=500, detail=f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")

app.add_middleware(LargeFileMiddleware)

# å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãŸã‚ã®CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸ã®ãƒ«ãƒ¼ãƒˆè¿½åŠ 
@app.get("/benchmark_test.html", response_class=HTMLResponse)
def benchmark_test():
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‚’é…ä¿¡"""
    try:
        with open("benchmark_test.html", "r", encoding="utf-8") as f:
            content = f.read()
        return HTMLResponse(content)
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

# Buffalo_lãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆInsightFaceï¼‰
MODEL_CONFIG = {
    "path": "w600k_r50.onnx",
    "name": "Buffalo_l WebFace600K ResNet50",
    "description": "WebFace600Kï¼ˆ60ä¸‡äººã€600ä¸‡æšï¼‰ã§è¨“ç·´ã•ã‚ŒãŸé«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ï¼ˆInsightFaceï¼‰",
    "input_name": "input.1",
    "input_size": (112, 112),
    "output_name": "683",
    "embedding_size": 512
}

# Buffalo_lãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
def initialize_model():
    model_path = MODEL_CONFIG["path"]
    
    if os.path.exists(model_path):
        try:
            session = onnxruntime.InferenceSession(
                model_path, 
                providers=['CPUExecutionProvider']
            )
            print(f"âœ… {MODEL_CONFIG['name']} èª­ã¿è¾¼ã¿å®Œäº†")
            return session
        except Exception as e:
            print(f"âŒ {MODEL_CONFIG['name']} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    else:
        print(f"âŒ è­¦å‘Š: {model_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None

# Buffalo_lãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ–
buffalo_session = initialize_model()

print("ğŸƒ Buffalo_l WebFace600K ãƒ¢ãƒ‡ãƒ«ï¼ˆInsightFaceï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™")

# InsightFace/Buffalo_lé¡”æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
BUFFALO_L_AVAILABLE = False
buffalo_l_app = None
try:
    from insightface.app import FaceAnalysis
    # Buffalo_lé¡”æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    buffalo_l_app = FaceAnalysis(
        providers=['CPUExecutionProvider'],
        allowed_modules=['detection'],
        name='buffalo_l'
    )
    # det_sizeã‚’æœ€é©è¨­å®šã«å¤‰æ›´ï¼ˆ320x320ï¼‰
    buffalo_l_app.prepare(ctx_id=0, det_size=(320, 320))
    print("âœ… Buffalo_lé¡”æ¤œå‡ºãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº† (det_size=320x320)")
    BUFFALO_L_AVAILABLE = True
except Exception as e:
    print(f"âš ï¸ Buffalo_lé¡”æ¤œå‡ºãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
    BUFFALO_L_AVAILABLE = False

def enhance_image_quality(image):
    """ç”»åƒå“è³ªã®å‘ä¸Šå‡¦ç†"""
    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å‡ç­‰åŒ–ï¼ˆæ˜åº¦æ”¹å–„ï¼‰
    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
    lab[:,:,0] = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)).apply(lab[:,:,0])
    enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
    
    # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ–ãƒ©ãƒ¼å¾Œã®ã‚·ãƒ£ãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°ã§ãƒã‚¤ã‚ºé™¤å»ã¨ã‚¨ãƒƒã‚¸å¼·èª¿
    blurred = cv2.GaussianBlur(enhanced, (3, 3), 0)
    sharpened = cv2.addWeighted(enhanced, 1.5, blurred, -0.5, 0)
    
    return sharpened


def detect_and_align_buffalo_l(image, save_crop=False, original_filename=None):
    """Buffalo_lé¡”æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹é¡”æ¤œå‡ºã¨ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ
    
    Args:
        image: å…¥åŠ›ç”»åƒ (OpenCVå½¢å¼)
        save_crop: åˆ‡ã‚Šå‡ºã—ç”»åƒã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
        original_filename: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¿å­˜æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆç”¨ï¼‰
    """
    if not BUFFALO_L_AVAILABLE or buffalo_l_app is None:
        return None
    
    try:
        # BGR -> RGBå¤‰æ›
        if len(image.shape) == 3 and image.shape[2] == 3:
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            rgb_image = image
        
        # Buffalo_lã§é¡”æ¤œå‡º
        faces = buffalo_l_app.get(rgb_image)
        
        if len(faces) == 0:
            return None
        
        # æœ€ã‚‚å¤§ãã„é¡”ã‚’é¸æŠï¼ˆå®‰å…¨ãªæ–¹æ³•ï¼‰
        best_face = None
        max_area = 0
        for face in faces:
            try:
                bbox = face.bbox
                if len(bbox) >= 4:
                    area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
                    if area > max_area:
                        max_area = area
                        best_face = face
            except Exception as e:
                print(f"âš ï¸ é¡”é¸æŠã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if best_face is None:
            print("âŒ æœ‰åŠ¹ãªé¡”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’å–å¾—
        bbox = best_face.bbox
        
        # bboxå½¢çŠ¶ãƒã‚§ãƒƒã‚¯
        if len(bbox) < 4:
            print(f"âŒ ç„¡åŠ¹ãªbboxå½¢çŠ¶: {bbox.shape}, æœ€ä½4ã¤ã®å€¤ãŒå¿…è¦")
            return None
        
        bbox = bbox.astype(int)
        x1, y1, x2, y2 = bbox[:4]  # æœ€åˆã®4ã¤ã®å€¤ã®ã¿ä½¿ç”¨
        
        # ãƒãƒ¼ã‚¸ãƒ³ã‚’è¿½åŠ 
        margin = 0.2
        width = x2 - x1
        height = y2 - y1
        x1 = max(0, int(x1 - width * margin))
        y1 = max(0, int(y1 - height * margin))
        x2 = min(image.shape[1], int(x2 + width * margin))
        y2 = min(image.shape[0], int(y2 + height * margin))
        
        # æ­£æ–¹å½¢ã«èª¿æ•´
        width = x2 - x1
        height = y2 - y1
        if width != height:
            size = max(width, height)
            center_x = x1 + width // 2
            center_y = y1 + height // 2
            x1 = max(0, center_x - size // 2)
            y1 = max(0, center_y - size // 2)
            x2 = min(image.shape[1], x1 + size)
            y2 = min(image.shape[0], y1 + size)
        
        # é¡”é ˜åŸŸã‚’åˆ‡ã‚Šå‡ºã—
        face_crop = image[y1:y2, x1:x2]
        
        # 112x112ã«ãƒªã‚µã‚¤ã‚º
        aligned_face = cv2.resize(face_crop, (112, 112))
        
        # åˆ‡ã‚Šå‡ºã—ç”»åƒã‚’ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if save_crop and original_filename:
            try:
                # ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºä¿
                crop_dir = "static/face_crops"
                os.makedirs(crop_dir, exist_ok=True)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆå…ƒãƒ•ã‚¡ã‚¤ãƒ«å + ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰
                import time
                timestamp = int(time.time() * 1000)  # ãƒŸãƒªç§’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
                base_name = os.path.splitext(os.path.basename(original_filename))[0]
                crop_filename = f"{crop_dir}/crop_{base_name}_{timestamp}.jpg"
                
                # å…ƒã®åˆ‡ã‚Šå‡ºã—ç”»åƒï¼ˆãƒªã‚µã‚¤ã‚ºå‰ï¼‰ã‚’ä¿å­˜
                cv2.imwrite(crop_filename, face_crop)
                
                # ãƒªã‚µã‚¤ã‚ºå¾Œã®ç”»åƒã‚‚ä¿å­˜
                aligned_filename = f"{crop_dir}/aligned_{base_name}_{timestamp}.jpg"
                cv2.imwrite(aligned_filename, aligned_face)
                
                print(f"ğŸ’¾ é¡”åˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜: {crop_filename}")
                print(f"ğŸ’¾ ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆç”»åƒä¿å­˜: {aligned_filename}")
                
            except Exception as e:
                print(f"âš ï¸ åˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"âœ… Buffalo_lé¡”æ¤œå‡ºæˆåŠŸ: ä¿¡é ¼åº¦={best_face.det_score:.3f}, bbox=({x1},{y1},{x2-x1},{y2-y1})")
        return aligned_face
        
    except Exception as e:
        print(f"âŒ Buffalo_lé¡”æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None

def detect_and_align_face(image_path, save_crop=False):
    """Buffalo_lé¡”æ¤œå‡ºãƒ»ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå‡¦ç†"""
    image = cv2.imread(image_path)
    if image is None:
        return None
    
    # Buffalo_lã«ã‚ˆã‚‹é¡”æ¤œå‡ºã®ã¿å®Ÿè¡Œ
    if BUFFALO_L_AVAILABLE:
        buffalo_result = detect_and_align_buffalo_l(
            image, 
            save_crop=save_crop, 
            original_filename=image_path
        )
        if buffalo_result is not None:
            return buffalo_result
        else:
            print("âŒ Buffalo_lé¡”æ¤œå‡ºå¤±æ•—")
            return None
    else:
        print("âŒ Buffalo_lé¡”æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return None

def preprocess_image_for_model(file_path, use_detection=True, save_crop=False):
    """Buffalo_lãƒ¢ãƒ‡ãƒ«ç”¨ã®å‰å‡¦ç†"""
    input_size = MODEL_CONFIG["input_size"]
    
    if use_detection:
        # é¡”æ¤œå‡ºã¨ã‚¯ãƒ­ãƒƒãƒ—ï¼ˆåˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
        face_image = detect_and_align_face(file_path, save_crop=save_crop)
        if face_image is None:
            return None
        
        # OpenCVç”»åƒã‚’PILã«å¤‰æ›
        face_image_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)
        img = Image.fromarray(face_image_rgb).convert('RGB').resize(input_size)
    else:
        with open(file_path, 'rb') as f:
            img = Image.open(f).convert('RGB').resize(input_size)
    
    img = np.asarray(img, dtype=np.float32)
    img = (img - 127.5) / 128.0
    img = np.transpose(img, (2, 0, 1))  # CHW
    img = np.expand_dims(img, axis=0)   # NCHW
    return img

def preprocess_image_simple(file):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªå‰å‡¦ç†ï¼ˆé¡”æ¤œå‡ºãªã—ï¼‰"""
    img = Image.open(file).convert('RGB').resize((112, 112))
    img = np.asarray(img, dtype=np.float32)
    img = (img - 127.5) / 128.0
    img = np.transpose(img, (2, 0, 1))  # CHW
    img = np.expand_dims(img, axis=0)   # NCHW
    return img

def preprocess_images_batch(file_paths, use_detection=True, batch_size=32, save_crop=False):
    """è¤‡æ•°ç”»åƒã®ãƒãƒƒãƒå‰å‡¦ç†"""
    input_size = MODEL_CONFIG["input_size"]
    
    batch_images = []
    valid_indices = []
    
    for idx, file_path in enumerate(file_paths):
        try:
            if use_detection:
                # é¡”æ¤œå‡ºã¨ã‚¯ãƒ­ãƒƒãƒ—ï¼ˆåˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
                face_image = detect_and_align_face(file_path, save_crop=save_crop)
                if face_image is None:
                    continue
                
                # OpenCVç”»åƒã‚’PILã«å¤‰æ›
                face_image_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)
                img = Image.fromarray(face_image_rgb).convert('RGB').resize(input_size)
            else:
                with open(file_path, 'rb') as f:
                    img = Image.open(f).convert('RGB').resize(input_size)
            
            img = np.asarray(img, dtype=np.float32)
            img = (img - 127.5) / 128.0
            img = np.transpose(img, (2, 0, 1))  # CHW
            
            batch_images.append(img)
            valid_indices.append(idx)
            
        except Exception as e:
            print(f"ãƒãƒƒãƒå‰å‡¦ç†ã‚¨ãƒ©ãƒ¼ [{idx}]: {e}")
            continue
    
    if not batch_images:
        return None, []
    
    # ãƒãƒƒãƒã«å¤‰æ›
    batch_array = np.stack(batch_images, axis=0)  # NCHW
    return batch_array, valid_indices

def calculate_optimal_batch_size(total_files, available_memory_gb=None):
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã«åŸºã¥ãæœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—"""
    if PSUTIL_AVAILABLE and available_memory_gb is None:
        memory = psutil.virtual_memory()
        available_memory_gb = memory.available / (1024**3)
    elif available_memory_gb is None:
        available_memory_gb = 4.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    # ãƒ¡ãƒ¢ãƒªã«åŸºã¥ããƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—
    # å„ç”»åƒã¯ç´„112x112x3x4 = 150KBã€ã•ã‚‰ã«å‰å‡¦ç†ã§2-3å€ã«ãªã‚‹ã¨ä»®å®š
    memory_per_image_mb = 0.5  # ä¿å®ˆçš„ãªè¦‹ç©ã‚‚ã‚Š
    max_batch_by_memory = int((available_memory_gb * 1024 * 0.3) / memory_per_image_mb)  # åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã®30%ã‚’ä½¿ç”¨
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«åŸºã¥ãèª¿æ•´
    if total_files <= 100:
        file_based_batch = min(16, total_files)
    elif total_files <= 500:
        file_based_batch = 32
    elif total_files <= 1000:
        file_based_batch = 64
    elif total_files <= 3000:
        file_based_batch = 256  # ã‚ˆã‚Šå¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚º
    else:
        file_based_batch = 512  # ã•ã‚‰ã«å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚º
    
    # ã‚ˆã‚Šä¿å®ˆçš„ãªå€¤ã‚’é¸æŠ
    optimal_batch = min(max_batch_by_memory, file_based_batch, 512)  # æœ€å¤§512ã«åˆ¶é™
    optimal_batch = max(optimal_batch, 16)  # æœ€å°16
    
    print(f"ğŸ“Š ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—: ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹={max_batch_by_memory}, ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹={file_based_batch}, æœ€é©={optimal_batch}")
    return optimal_batch

def get_embedding_batch(file_paths, use_detection=True, batch_size=None):
    """ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹é«˜é€Ÿãªç‰¹å¾´é‡æŠ½å‡º"""
    if buffalo_session is None:
        return None, []
    
    # è‡ªå‹•ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
    if batch_size is None:
        batch_size = calculate_optimal_batch_size(len(file_paths))
    
    session = buffalo_session
    input_name = MODEL_CONFIG["input_name"]
    
    all_embeddings = []
    all_valid_indices = []
    processed_count = 0
    
    print(f"ğŸš€ ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {len(file_paths)}ãƒ•ã‚¡ã‚¤ãƒ«, ãƒãƒƒãƒã‚µã‚¤ã‚º={batch_size}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒãƒã‚µã‚¤ã‚ºã”ã¨ã«åˆ†å‰²
    for i in range(0, len(file_paths), batch_size):
        batch_files = file_paths[i:i + batch_size]
        batch_num = (i // batch_size) + 1
        total_batches = (len(file_paths) + batch_size - 1) // batch_size
        
        try:
            # ãƒãƒƒãƒå‰å‡¦ç†ï¼ˆåˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜ã‚’æœ‰åŠ¹åŒ–ï¼‰
            batch_images, valid_indices = preprocess_images_batch(
                batch_files, use_detection, batch_size, save_crop=True
            )
            
            if batch_images is None:
                print(f"âš ï¸ ãƒãƒƒãƒ {batch_num}/{total_batches}: å‡¦ç†å¯èƒ½ãªç”»åƒãªã—")
                continue
            
            # Buffalo_lãƒ¢ãƒ‡ãƒ«ã¯ãƒãƒƒãƒã‚µã‚¤ã‚º1ã®ã¿å¯¾å¿œã®ãŸã‚ã€1æšãšã¤æ¨è«–
            embeddings = []
            for idx, single_image in enumerate(batch_images):
                try:
                    single_input = np.expand_dims(single_image, axis=0)  # (1, C, H, W)
                    single_embedding = session.run(None, {input_name: single_input})[0]
                    if single_embedding.shape[0] == 1:  # æœŸå¾…ã•ã‚Œã‚‹å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
                        embeddings.append(single_embedding[0])  # ãƒãƒƒãƒæ¬¡å…ƒã‚’é™¤å»
                    else:
                        print(f"âš ï¸ äºˆæœŸã—ãªã„åŸ‹ã‚è¾¼ã¿å½¢çŠ¶: {single_embedding.shape}")
                        continue
                except Exception as e:
                    print(f"âŒ å˜ä¸€ç”»åƒæ¨è«–ã‚¨ãƒ©ãƒ¼ [{idx}]: {e}")
                    continue
            
            if not embeddings:
                print(f"âš ï¸ ãƒãƒƒãƒ {batch_num}/{total_batches}: æ¨è«–å¯èƒ½ãªç”»åƒãªã—")
                continue
                
            embeddings = np.array(embeddings)
            
            # æ­£è¦åŒ–
            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
            
            # çµæœã‚’ä¿å­˜ï¼ˆå…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª¿æ•´ï¼‰
            adjusted_indices = [i + idx for idx in valid_indices]
            all_embeddings.extend(embeddings)
            all_valid_indices.extend(adjusted_indices)
            
            processed_count += len(embeddings)
            
            # é€²æ—è¡¨ç¤º
            if batch_num % 10 == 0 or batch_num == total_batches:
                progress = (processed_count / len(file_paths)) * 100
                print(f"ğŸ“ˆ ãƒãƒƒãƒ {batch_num}/{total_batches} å®Œäº†: {processed_count}/{len(file_paths)} ({progress:.1f}%)")
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå¤§é‡å‡¦ç†æ™‚ï¼‰
            if batch_num % 50 == 0:
                gc.collect()
            
        except Exception as e:
            print(f"âŒ ãƒãƒƒãƒæ¨è«–ã‚¨ãƒ©ãƒ¼ (batch {batch_num}): {e}")
            # ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’åŠåˆ†ã«ã—ã¦å†è©¦è¡Œ
            if "memory" in str(e).lower() or "allocation" in str(e).lower():
                print(f"ğŸ”„ ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼æ¤œå‡ºã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’åŠåˆ†ã«å‰Šæ¸›: {batch_size} â†’ {batch_size//2}")
                return get_embedding_batch(file_paths, use_detection, max(batch_size//2, 4))
            continue
    
    print(f"âœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(all_embeddings)}å€‹ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ")
    return all_embeddings, all_valid_indices

def get_embedding_buffalo(file_path, use_detection=True):
    """Buffalo_lãƒ¢ãƒ‡ãƒ«ã§åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—"""
    if buffalo_session is None:
        return {
            'embedding': None,
            'error': 'Buffalo_lãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“',
            'processing_time': 0
        }
    
    start_time = time.time()
    try:
        # å‰å‡¦ç†
        img = preprocess_image_for_model(file_path, use_detection)
        
        if img is None:
            return {
                'embedding': None,
                'error': 'ç”»åƒå‡¦ç†ã«å¤±æ•—',
                'processing_time': 0
            }
        
        # æ¨è«–å®Ÿè¡Œ
        input_name = MODEL_CONFIG["input_name"]
        embedding = buffalo_session.run(None, {input_name: img})[0]
        embedding = embedding[0]
        
        # æ­£è¦åŒ–
        embedding = embedding / np.linalg.norm(embedding)
        
        processing_time = (time.time() - start_time) * 1000  # ms
        
        return {
            'embedding': embedding,
            'error': None,
            'processing_time': processing_time
        }
        
    except Exception as e:
        return {
            'embedding': None,
            'error': str(e),
            'processing_time': (time.time() - start_time) * 1000
        }

def get_embedding_single(file_path, use_detection=True):
    """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆãƒãƒƒãƒå‡¦ç†ãªã—ï¼‰"""
    if buffalo_session is None:
        return None
    
    try:
        # 1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤å‡¦ç†
        img = preprocess_image_for_model(file_path, use_detection)
        
        if img is None:
            return None
        
        # æ¨è«–å®Ÿè¡Œï¼ˆ1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤ï¼‰
        input_name = MODEL_CONFIG["input_name"]
        embedding = buffalo_session.run(None, {input_name: img})[0]
        embedding = embedding[0]
        
        # æ­£è¦åŒ–
        embedding = embedding / np.linalg.norm(embedding)
        
        return embedding
        
    except Exception as e:
        print(f"âŒ å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def cosine_similarity(a, b):
    return float(np.dot(a, b))

def adaptive_threshold(cosine_sim, euclidean_dist, base_threshold=0.45):
    """é©å¿œçš„é–¾å€¤èª¿æ•´"""
    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãŒé«˜ã„å ´åˆã¯é–¾å€¤ã‚’ä¸‹ã’ã‚‹
    if cosine_sim > 0.8:
        return base_threshold * 0.9
    elif cosine_sim > 0.6:
        return base_threshold * 0.95
    else:
        return base_threshold

def ensemble_verification(embeddings1, embeddings2):
    """è¤‡æ•°ã®æ‰‹æ³•ã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œè¨¼"""
    results = {}
    
    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
    cosine_sim = cosine_similarity(embeddings1, embeddings2)
    
    # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢
    euclidean_dist = float(np.linalg.norm(embeddings1 - embeddings2))
    
    # L1è·é›¢ï¼ˆãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ï¼‰
    l1_dist = float(np.sum(np.abs(embeddings1 - embeddings2)))
    
    # æ­£è¦åŒ–ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢
    norm_euclidean = euclidean_dist / (np.linalg.norm(embeddings1) + np.linalg.norm(embeddings2))
    
    # é©å¿œçš„é–¾å€¤
    adaptive_thresh = adaptive_threshold(cosine_sim, euclidean_dist)
    
    results = {
        'cosine_similarity': cosine_sim,
        'euclidean_distance': euclidean_dist,
        'l1_distance': l1_dist,
        'normalized_euclidean': norm_euclidean,
        'adaptive_threshold': adaptive_thresh,
        'is_same_adaptive': cosine_sim > adaptive_thresh,
        'confidence_score': min(cosine_sim * 1.2, 1.0)  # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    }
    
    return results

def compare_buffalo_faces(file_path1, file_path2):
    """Buffalo_lãƒ¢ãƒ‡ãƒ«ã§2ã¤ã®é¡”ã‚’æ¯”è¼ƒ"""
    # å„ç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    embedding1 = get_embedding_buffalo(file_path1, use_detection=True)
    embedding2 = get_embedding_buffalo(file_path2, use_detection=True)
    
    if (embedding1['embedding'] is not None and 
        embedding2['embedding'] is not None):
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œè¨¼
        ensemble_result = ensemble_verification(
            embedding1['embedding'],
            embedding2['embedding']
        )
        
        return {
            'model_info': MODEL_CONFIG,
            'ensemble_result': ensemble_result,
            'processing_time': (embedding1['processing_time'] + 
                               embedding2['processing_time']),
            'error': None
        }
    else:
        error_msg = embedding1.get('error', 'Unknown error') + '; ' + embedding2.get('error', 'Unknown error')
        return {
            'model_info': MODEL_CONFIG,
            'ensemble_result': None,
            'processing_time': 0,
            'error': error_msg
        }


def save_temp_image(file):
    file.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
    try:
        # PILã§ç”»åƒã‚’é–‹ã„ã¦å½¢å¼ã‚’ç¢ºèªãƒ»å¤‰æ›
        img = Image.open(file)
        
        # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å½¢å¼ã‚’ç¢ºèª
        original_format = img.format
        print(f"å…ƒã®ç”»åƒå½¢å¼: {original_format}")
        
        # é€æ˜åº¦ãƒãƒ£ãƒ³ãƒãƒ«ãŒã‚ã‚‹å ´åˆã¯èƒŒæ™¯ã‚’ç™½ã«è¨­å®šã—ã¦RGBã«å¤‰æ›
        if img.mode in ('RGBA', 'LA', 'P'):
            # é€æ˜åº¦ã‚’æŒã¤ç”»åƒã®å ´åˆã€ç™½èƒŒæ™¯ã§RGBã«å¤‰æ›
            background = Image.new('RGB', img.size, (255, 255, 255))
            if img.mode == 'P':
                img = img.convert('RGBA')
            background.paste(img, mask=img.split()[-1] if img.mode in ('RGBA', 'LA') else None)
            img = background
        else:
            img = img.convert('RGB')  # RGBã«å¤‰æ›ï¼ˆã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ãªã©ã‚‚çµ±ä¸€ï¼‰
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp:
            img.save(tmp.name, 'JPEG', quality=95)
            print(f"ç”»åƒã‚’å¤‰æ›ã—ã¦ä¿å­˜: {original_format} -> JPEG")
            return tmp.name
            
    except Exception as e:
        print(f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã®æ–¹æ³•ã§ãƒã‚¤ãƒŠãƒªä¿å­˜
        try:
            file.seek(0)
            with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp:
                tmp.write(file.read())
                print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒã‚¤ãƒŠãƒªä¿å­˜ã‚’ä½¿ç”¨")
                return tmp.name
        except Exception as fallback_error:
            print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã§ã‚‚ã‚¨ãƒ©ãƒ¼: {str(fallback_error)}")
            raise fallback_error

def verify_faces(file1, file2):
    temp_path1 = save_temp_image(file1)
    temp_path2 = save_temp_image(file2)
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if not os.path.exists(temp_path1) or not os.path.exists(temp_path2):
            raise ValueError("ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        if os.path.getsize(temp_path1) == 0 or os.path.getsize(temp_path2) == 0:
            raise ValueError("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™")
        
        # ArcFaceã§ã®æ¤œè¨¼
        result_arcface_cosine = DeepFace.verify(temp_path1, temp_path2, model_name='ArcFace', distance_metric='cosine')
        result_arcface_euclidean = DeepFace.verify(temp_path1, temp_path2, model_name='ArcFace', distance_metric='euclidean')
        
        # DeepFaceã‹ã‚‰ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾— (ArcFace)
        embedding1_arcface = DeepFace.represent(temp_path1, model_name='ArcFace')[0]['embedding']
        embedding2_arcface = DeepFace.represent(temp_path2, model_name='ArcFace')[0]['embedding']
        
        # æ‰‹å‹•ã§ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®— (ArcFace)
        embedding1_arcface = np.array(embedding1_arcface)
        embedding2_arcface = np.array(embedding2_arcface)
        manual_cosine_sim_arcface = float(np.dot(embedding1_arcface, embedding2_arcface) / (np.linalg.norm(embedding1_arcface) * np.linalg.norm(embedding2_arcface)))
        manual_cosine_dist_arcface = 1 - manual_cosine_sim_arcface
        
        return {
            'arcface': {
                'cosine': result_arcface_cosine,
                'euclidean': result_arcface_euclidean,
                'embeddings': {
                    'emb1': embedding1_arcface.tolist()[:20],  # æœ€åˆã®20æ¬¡å…ƒè¡¨ç¤º
                    'emb2': embedding2_arcface.tolist()[:20],
                    'manual_cosine_similarity': f"{manual_cosine_sim_arcface:.4f}",
                    'manual_cosine_distance': f"{manual_cosine_dist_arcface:.4f}",
                    'embedding_dims': len(embedding1_arcface)
                }
            }
        }
    except Exception as e:
        print(f"DeepFaceå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        print(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«1: {temp_path1} (å­˜åœ¨: {os.path.exists(temp_path1)})")
        print(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«2: {temp_path2} (å­˜åœ¨: {os.path.exists(temp_path2)})")
        if os.path.exists(temp_path1):
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«1ã‚µã‚¤ã‚º: {os.path.getsize(temp_path1)}")
        if os.path.exists(temp_path2):
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«2ã‚µã‚¤ã‚º: {os.path.getsize(temp_path2)}")
        
        # DeepFaceãŒå¤±æ•—ã—ãŸå ´åˆã¯ãƒ€ãƒŸãƒ¼ã®çµæœã‚’è¿”ã™
        print("DeepFaceãŒå¤±æ•—ã—ãŸãŸã‚ã€ãƒ€ãƒŸãƒ¼çµæœã‚’è¿”ã—ã¾ã™")
        return {
            'arcface': {
                'cosine': {
                    'distance': 0.0,
                    'threshold': 0.68,
                    'verified': False
                },
                'euclidean': {
                    'distance': 0.0,
                    'threshold': 4.15,
                    'verified': False
                },
                'embeddings': {
                    'emb1': [0.0] * 10,
                    'emb2': [0.0] * 10,
                    'manual_cosine_similarity': "0.0000",
                    'manual_cosine_distance': "1.0000",
                    'embedding_dims': 512
                }
            }
        }
    finally:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‰Šé™¤
        if os.path.exists(temp_path1):
            os.unlink(temp_path1)
        if os.path.exists(temp_path2):
            os.unlink(temp_path2)

@app.get("/", response_class=HTMLResponse)
def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request, "result": None})

@app.post("/verify", response_class=HTMLResponse)
def verify(request: Request, file1: UploadFile = File(...), file2: UploadFile = File(...)):
    # ç”»åƒä¿å­˜
    os.makedirs("static/uploads", exist_ok=True)
    filename1 = f"static/uploads/{uuid.uuid4().hex}_{file1.filename}"
    filename2 = f"static/uploads/{uuid.uuid4().hex}_{file2.filename}"
    with open(filename1, "wb") as buffer1:
        shutil.copyfileobj(file1.file, buffer1)
    with open(filename2, "wb") as buffer2:
        shutil.copyfileobj(file2.file, buffer2)

    # DeepFace verification
    file1.file.seek(0)
    file2.file.seek(0)
    deepface_results = verify_faces(file1.file, file2.file)
    
    # Buffalo_lé¡”èªè­˜å‡¦ç†
    buffalo_comparison = compare_buffalo_faces(filename1, filename2)
    
    # Buffalo_låŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—
    emb1_buffalo = get_embedding_buffalo(filename1, use_detection=True)
    emb2_buffalo = get_embedding_buffalo(filename2, use_detection=True)
    
    if (emb1_buffalo['embedding'] is not None and 
        emb2_buffalo['embedding'] is not None):
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œè¨¼ã‚’ä½¿ç”¨
        ensemble_results = ensemble_verification(
            emb1_buffalo['embedding'], 
            emb2_buffalo['embedding']
        )
        similarity_buffalo = ensemble_results['cosine_similarity']
        is_same_buffalo = ensemble_results['is_same_adaptive']
        confidence_score = ensemble_results['confidence_score']
    else:
        similarity_buffalo = 0.0
        is_same_buffalo = False
        confidence_score = 0.0
        ensemble_results = {
            'cosine_similarity': 0.0,
            'euclidean_distance': 0.0,
            'l1_distance': 0.0,
            'normalized_euclidean': 0.0,
            'adaptive_threshold': 0.45,
            'confidence_score': 0.0
        }
    
    # Buffalo_låŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®è©³ç´°æƒ…å ±
    buffalo_embedding_info = {
        'emb1': emb1_buffalo['embedding'].tolist()[:20] if emb1_buffalo['embedding'] is not None else [],
        'emb2': emb2_buffalo['embedding'].tolist()[:20] if emb2_buffalo['embedding'] is not None else [],
        'embedding_dims': MODEL_CONFIG['embedding_size'],
        'emb1_norm': float(np.linalg.norm(emb1_buffalo['embedding'])) if emb1_buffalo['embedding'] is not None else 0.0,
        'emb2_norm': float(np.linalg.norm(emb2_buffalo['embedding'])) if emb2_buffalo['embedding'] is not None else 0.0
    }
    
    result = {
        "deepface_arcface": {
            "cosine": {
                "distance": f"{deepface_results['arcface']['cosine']['distance']:.4f}",
                "similarity": f"{1 - deepface_results['arcface']['cosine']['distance']:.4f}",
                "is_same": deepface_results['arcface']['cosine']['verified'],
                "threshold": f"{deepface_results['arcface']['cosine']['threshold']:.4f}"
            },
            "euclidean": {
                "distance": f"{deepface_results['arcface']['euclidean']['distance']:.4f}",
                "similarity": f"{1 - deepface_results['arcface']['euclidean']['distance']:.4f}",
                "is_same": deepface_results['arcface']['euclidean']['verified'],
                "threshold": f"{deepface_results['arcface']['euclidean']['threshold']:.4f}"
            },
            "embeddings": deepface_results['arcface']['embeddings']
        },
        "buffalo_l": {
            "similarity": f"{similarity_buffalo:.4f}",
            "is_same": is_same_buffalo,
            "adaptive_threshold": f"{ensemble_results.get('adaptive_threshold', 0.5):.4f}",
            "confidence_score": f"{confidence_score:.4f}",
            "euclidean_distance": f"{ensemble_results.get('euclidean_distance', 0.0):.4f}",
            "l1_distance": f"{ensemble_results.get('l1_distance', 0.0):.4f}",
            "normalized_euclidean": f"{ensemble_results.get('normalized_euclidean', 0.0):.4f}",
            "embeddings": buffalo_embedding_info,
            "processing_time": f"{emb1_buffalo.get('processing_time', 0) + emb2_buffalo.get('processing_time', 0):.1f}ms"
        },
        "img1_path": "/" + filename1,
        "img2_path": "/" + filename2,
        "buffalo_comparison": buffalo_comparison,
        "model_info": {
            "deepface_arcface": "ArcFace (DeepFace implementation)",
            "buffalo_l": MODEL_CONFIG['name'],
            "description": MODEL_CONFIG['description']
        }
    }
    return templates.TemplateResponse("index.html", {"request": request, "result": result})

@app.post("/compare_folder_stream")
async def compare_folder_stream(
    query_image: UploadFile = File(...),
    folder_images: List[UploadFile] = File(...)
):
    """1å¯¾Næ¯”è¼ƒã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—æ›´æ–°"""
    async def generate_stream():
        try:
            # åˆæœŸåŒ–ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            yield f"data: {json.dumps({'type': 'init', 'total': len(folder_images)}, ensure_ascii=False)}\n\n"
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†å®Ÿè£…ï¼ˆç°¡ç•¥ç‰ˆï¼‰
            for i, file in enumerate(folder_images):
                progress = {
                    'type': 'progress',
                    'current': i + 1,
                    'total': len(folder_images),
                    'percentage': ((i + 1) / len(folder_images)) * 100
                }
                yield f"data: {json.dumps(progress, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.01)  # éåŒæœŸå‡¦ç†ã®ãŸã‚
            
            # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            yield f"data: {json.dumps({'type': 'complete'}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            error_msg = {'type': 'error', 'message': str(e)}
            yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache"}
    )

@app.post("/compare_folder_unlimited")
async def compare_folder_unlimited(request: Request):
    """ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™ã‚’å®Œå…¨ã«å›é¿ã™ã‚‹å°‚ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆ3520ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰"""
    try:
        print("ğŸš€ åˆ¶é™è§£é™¤å°‚ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆèµ·å‹•")
        
        # ç”Ÿã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã‚’ç›´æ¥å‡¦ç†
        content_type = request.headers.get('content-type', '')
        print(f"ğŸ“‹ Content-Type: {content_type}")
        
        if not content_type.startswith('multipart/form-data'):
            raise HTTPException(status_code=400, detail="multipart/form-dataãŒå¿…è¦ã§ã™")
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆè§£æï¼ˆåˆ¶é™ãªã—ï¼‰
        from starlette.datastructures import FormData, UploadFile
        import email
        from email.message import EmailMessage
        import io
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã‚’å–å¾—
        body = await request.body()
        print(f"ğŸ“¦ å—ä¿¡ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(body) / 1024 / 1024:.1f}MB")
        
        # ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆå¢ƒç•Œã‚’å–å¾—
        boundary = None
        for param in content_type.split(';'):
            if 'boundary=' in param:
                boundary = param.split('boundary=')[1].strip()
                break
        
        if not boundary:
            raise HTTPException(status_code=400, detail="ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆå¢ƒç•ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        print(f"ğŸ” å¢ƒç•Œæ–‡å­—åˆ—: {boundary[:20]}...")
        
        # ç°¡æ˜“ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆè§£æ
        parts = body.split(f'--{boundary}'.encode())
        print(f"ğŸ“ æ¤œå‡ºã•ã‚ŒãŸãƒ‘ãƒ¼ãƒˆæ•°: {len(parts)}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå¤šã„å ´åˆã¯å°‚ç”¨å‡¦ç†ã«è»¢é€
        if len(parts) > 1000:
            print(f"âœ… å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: {len(parts)}ãƒ‘ãƒ¼ãƒˆæ¤œå‡º")
            # å®Ÿéš›ã®å‡¦ç†ã‚’compare_folder_internalã«å§”è­²
            return JSONResponse({
                "message": f"åˆ¶é™è§£é™¤ãƒ¢ãƒ¼ãƒ‰ã§{len(parts)}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ä¿¡ã—ã¾ã—ãŸ",
                "file_count": len(parts),
                "status": "ready_for_processing"
            })
        else:
            # é€šå¸¸å‡¦ç†
            return JSONResponse({
                "message": f"é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§{len(parts)}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ä¿¡ã—ã¾ã—ãŸ",
                "file_count": len(parts)
            })
            
    except Exception as e:
        print(f"âŒ åˆ¶é™è§£é™¤ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")

@app.post("/compare_folder_large")
async def compare_folder_large(
    query_image: UploadFile = File(...),
    folder_images: List[UploadFile] = File(...),
    chunk_size: int = Form(1000)  # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’ãƒ•ã‚©ãƒ¼ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æŒ‡å®šï¼ˆåˆ¶é™è§£é™¤ï¼‰
):
    """å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å°‚ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å¯¾å¿œ"""
    print(f"ğŸš€ å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {len(folder_images)}ãƒ•ã‚¡ã‚¤ãƒ«ã€ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º={chunk_size}")
    
    # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿å‡¦ç†ã—ã¦ã€æ®‹ã‚Šã¯åˆ¥é€”å‡¦ç†ã™ã‚‹ãŸã‚ã®è¨­è¨ˆ
    if len(folder_images) > chunk_size:
        # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿å‡¦ç†
        first_chunk = folder_images[:chunk_size]
        print(f"ğŸ“¦ ç¬¬1ãƒãƒ£ãƒ³ã‚¯å‡¦ç†: {len(first_chunk)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        # é€šå¸¸ã®å‡¦ç†é–¢æ•°ã‚’å‘¼ã³å‡ºã—
        return await compare_folder_internal(query_image, first_chunk, is_chunk=True)
    else:
        return await compare_folder_internal(query_image, folder_images)

async def compare_folder_internal(
    query_image: UploadFile,
    folder_images: List[UploadFile],
    is_chunk: bool = False
):
    """å†…éƒ¨å‡¦ç†é–¢æ•° - å®Ÿéš›ã®æ¯”è¼ƒå‡¦ç†ã‚’å®Ÿè¡Œ"""
    start_time = time.time()
    
    # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
    print(f"ğŸ” å†…éƒ¨å‡¦ç†é–‹å§‹: query_image={query_image.filename if query_image else 'None'}")
    print(f"ğŸ” ãƒ•ã‚©ãƒ«ãƒ€ç”»åƒæ•°: {len(folder_images) if folder_images else 0}")
    print(f"ğŸ” ãƒãƒ£ãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰: {is_chunk}")
    
    try:
        # æ—¢å­˜ã®å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚³ãƒ”ãƒ¼ï¼‰
        return await _process_folder_comparison(query_image, folder_images, start_time, is_chunk)
    except Exception as e:
        import traceback
        error_message = f'å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        error_traceback = traceback.format_exc()
        print(f"âŒ å†…éƒ¨å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_message}")
        print(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼è©³ç´°:\n{error_traceback}")
        
        return JSONResponse(
            status_code=500,
            content={
                'error': error_message,
                'error_type': type(e).__name__,
                'query_image': query_image.filename if query_image else 'Unknown',
                'total_files': len(folder_images) if folder_images else 0,
                'processing_status': 'failed',
                'is_chunk': is_chunk
            }
        )

async def _process_folder_comparison(query_image, folder_images, start_time, is_chunk=False):
    """ãƒ•ã‚©ãƒ«ãƒ€æ¯”è¼ƒã®å®Ÿéš›ã®å‡¦ç†"""
    
    # åŸºæœ¬çš„ãªå…¥åŠ›æ¤œè¨¼
    if not query_image:
        return JSONResponse(
            status_code=400,
            content={'error': 'ã‚¯ã‚¨ãƒªç”»åƒãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“', 'processing_status': 'failed'}
        )
    
    if not folder_images or len(folder_images) == 0:
        return JSONResponse(
            status_code=400,
            content={'error': 'ãƒ•ã‚©ãƒ«ãƒ€ç”»åƒãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“', 'processing_status': 'failed'}
        )
    
    # å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãƒ­ã‚°ã¨æœ€é©åŒ–è¨­å®š
    total_files = len(folder_images)
    print(f"ğŸ”¥ 1å¯¾Næ¤œç´¢é–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†äºˆå®š")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèªï¼ˆ3520ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰
    cpu_count = multiprocessing.cpu_count()
    if PSUTIL_AVAILABLE:
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        available_gb = memory.available / (1024**3)
        used_pct = memory.percent
        print(f"ğŸ–¥ï¸  ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹: CPU {cpu_count}ã‚³ã‚¢, ãƒ¡ãƒ¢ãƒª {memory_gb:.1f}GB (ä½¿ç”¨ç‡{used_pct:.1f}%, åˆ©ç”¨å¯èƒ½{available_gb:.1f}GB)")
        
        # ãƒ¡ãƒ¢ãƒªä¸è¶³è­¦å‘Š
        if used_pct > 80:
            print(f"âš ï¸  ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„ã§ã™ ({used_pct:.1f}%) - å‡¦ç†ã‚’è»½é‡åŒ–ã—ã¾ã™")
        if available_gb < 2:
            print(f"âš ï¸  åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªãŒå°‘ãªã„ã§ã™ ({available_gb:.1f}GB) - ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›ã—ã¾ã™")
    else:
        print(f"ğŸ–¥ï¸  ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹: CPU {cpu_count}ã‚³ã‚¢, ãƒ¡ãƒ¢ãƒªæƒ…å ±ä¸æ˜")
    
    # æœ€é©åŒ–ã•ã‚ŒãŸå‡¦ç†è¨­å®š
    print(f"ğŸ“‹ æœ€é©åŒ–å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    use_multiprocessing = False
    batch_size = calculate_optimal_batch_size(total_files)  # æœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨ˆç®—
    max_workers = 1
    memory_cleanup_interval = 100
    chunk_processing = False
    
    print(f"æœ€é©åŒ–è¨­å®š: ãƒãƒƒãƒã‚µã‚¤ã‚º={batch_size}, ä¸¦åˆ—æ•°={max_workers}, ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚·ãƒ³ã‚°={use_multiprocessing}")
    if 'chunk_processing' in locals() and chunk_processing:
        print(f"æ®µéšçš„å‡¦ç†: {chunk_size}ãƒ•ã‚¡ã‚¤ãƒ«æ¯ã«åˆ†å‰²å‡¦ç†")
    
    # ã‚¯ã‚¨ãƒªç”»åƒã‚’ä¿å­˜
    os.makedirs("static/temp", exist_ok=True)
    query_filename = f"static/temp/query_{uuid.uuid4().hex}_{query_image.filename}"
    with open(query_filename, "wb") as buffer:
        shutil.copyfileobj(query_image.file, buffer)
    
    print(f"ã‚¯ã‚¨ãƒªç”»åƒä¿å­˜å®Œäº†: {query_filename}")
    
    # Buffalo_lãƒ¢ãƒ‡ãƒ«ã§ã‚¯ã‚¨ãƒªç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    query_embedding = get_embedding_buffalo(query_filename, use_detection=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å‡¦ç†ã‚’å®Ÿè¡Œ
    file_info_list = await _save_files_individually(folder_images)
    
    print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {len(file_info_list)}ä»¶")
    
    # ä¿å­˜ã‚¨ãƒ©ãƒ¼ãŒãªã„ã‹ãƒã‚§ãƒƒã‚¯
    failed_saves = [f for f in file_info_list if f.get('error')]
    if failed_saves:
        print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {len(failed_saves)}ä»¶")
        for failed in failed_saves[:10]:  # æœ€åˆã®10ä»¶ã¾ã§è¡¨ç¤ºï¼ˆå…¨ä»¶ã¯å†—é•·ãªãŸã‚ï¼‰
            print(f"  - {failed['original_name']}: {failed.get('error', 'Unknown error')}")
    
    # æˆåŠŸã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å‡¦ç†å¯¾è±¡ã¨ã™ã‚‹
    valid_file_info_list = [f for f in file_info_list if not f.get('error') and f.get('filename')]
    print(f"ğŸ“Š å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(valid_file_info_list)}ä»¶")
    
    if not valid_file_info_list:
        return JSONResponse(
            status_code=400,
            content={
                'error': 'ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ',
                'failed_files': len(failed_saves),
                'processing_status': 'failed'
            }
        )
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªé †æ¬¡å‡¦ç†ã‚’å®Ÿè¡Œ
    print(f"ğŸ”„ é †æ¬¡å‡¦ç†é–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    results = await _execute_comparison_buffalo(query_embedding, valid_file_info_list, batch_size, start_time)
    
    # çµæœã®æ•´ç†ã¨è¿”å´
    return _format_comparison_results(results, query_image, total_files, valid_file_info_list, start_time, is_chunk)

async def _save_files_individually(folder_images):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å‡¦ç†ï¼ˆæœ€é©åŒ–ãªã—ï¼‰"""
    file_info_list = []
    temp_dir = "static/temp"
    os.makedirs(temp_dir, exist_ok=True)
    
    total_files = len(folder_images)
    print(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # 1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤é †æ¬¡ä¿å­˜
    for idx, folder_image in enumerate(folder_images):
        try:
            # é€²æ—è¡¨ç¤º
            if idx % 100 == 0 or idx == total_files - 1:
                progress_pct = (idx + 1) / total_files * 100
                print(f"ğŸ“‹ ä¿å­˜ä¸­: {idx + 1}/{total_files} ({progress_pct:.1f}%)")
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            safe_name = f"file_{idx}_{uuid.uuid4().hex[:8]}.jpg"
            file_path = f"{temp_dir}/{safe_name}"
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            folder_image.file.seek(0)
            content = await folder_image.read()
            
            with open(file_path, "wb") as f:
                f.write(content)
            
            file_info_list.append({
                'filename': file_path,
                'original_name': folder_image.filename,
                'index': idx
            })
            
        except Exception as e:
            print(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼ [{idx}]: {e}")
            file_info_list.append({
                'filename': None,
                'original_name': folder_image.filename or f"image_{idx}",
                'index': idx,
                'error': str(e)
            })
    
    print(f"âœ… ä¿å­˜å®Œäº†: {len(file_info_list)}ãƒ•ã‚¡ã‚¤ãƒ«")
    return file_info_list

async def _execute_chunked_comparison(query_embeddings, valid_file_info_list, selected_models, use_multiprocessing, batch_size, max_workers, memory_cleanup_interval, start_time, query_filename, chunk_size):
    """æ®µéšçš„æ¯”è¼ƒå‡¦ç†ã®å®Ÿè¡Œï¼ˆ3530ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰"""
    all_results = []
    total_files = len(valid_file_info_list)
    processed_files = 0
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
    for chunk_idx in range(0, total_files, chunk_size):
        chunk_end = min(chunk_idx + chunk_size, total_files)
        current_chunk = valid_file_info_list[chunk_idx:chunk_end]
        chunk_number = (chunk_idx // chunk_size) + 1
        total_chunks = (total_files + chunk_size - 1) // chunk_size
        
        print(f"ğŸ“¦ ãƒãƒ£ãƒ³ã‚¯ {chunk_number}/{total_chunks} å‡¦ç†ä¸­: {len(current_chunk)}ãƒ•ã‚¡ã‚¤ãƒ« ({chunk_idx+1}-{chunk_end})")
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
        chunk_results = await _execute_comparison(query_embeddings, current_chunk, selected_models, use_multiprocessing, batch_size, max_workers, memory_cleanup_interval, start_time, query_filename)
        all_results.extend(chunk_results)
        
        processed_files += len(current_chunk)
        progress_pct = (processed_files / total_files) * 100
        print(f"âœ… ãƒãƒ£ãƒ³ã‚¯ {chunk_number} å®Œäº†: ç´¯è¨ˆ {processed_files}/{total_files} ({progress_pct:.1f}%)")
        
        # ãƒãƒ£ãƒ³ã‚¯é–“ã§ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if chunk_number < total_chunks:  # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã§ãªã„å ´åˆ
            print("ğŸ§¹ ãƒãƒ£ãƒ³ã‚¯é–“ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ")
            gc.collect()
            await asyncio.sleep(0.2)  # çŸ­ã„ä¼‘æ†©
    
    print(f"ğŸ‰ æ®µéšçš„å‡¦ç†å®Œäº†: å…¨{total_chunks}ãƒãƒ£ãƒ³ã‚¯, {total_files}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ¸ˆã¿")
    return all_results

async def _execute_comparison_buffalo(query_embedding, valid_file_info_list, batch_size, start_time):
    """Buffalo_lãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒãƒƒãƒå‡¦ç†æ¯”è¼ƒ"""
    total_files = len(valid_file_info_list)
    
    print(f"ğŸš€ Buffalo_l ãƒãƒƒãƒå‡¦ç†æ¯”è¼ƒé–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # ãƒãƒƒãƒå‡¦ç†ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸€æ‹¬å–å¾—
    target_file_paths = [file_info['filename'] for file_info in valid_file_info_list]
    
    print(f"ğŸ“Š ãƒãƒƒãƒç‰¹å¾´é‡æŠ½å‡ºé–‹å§‹... (ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size})")
    target_embeddings, valid_indices = get_embedding_batch(
        target_file_paths, 
        use_detection=True,
        batch_size=batch_size
    )
    
    if not target_embeddings:
        print("âŒ ãƒãƒƒãƒç‰¹å¾´é‡æŠ½å‡ºã«å¤±æ•—")
        return []
    
    print(f"âœ… ãƒãƒƒãƒç‰¹å¾´é‡æŠ½å‡ºå®Œäº†: {len(target_embeddings)}å€‹ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«")
    
    # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    query_emb = query_embedding.get('embedding')
    if query_emb is None:
        print("âŒ ã‚¯ã‚¨ãƒªç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    
    print(f"ğŸ”„ é¡ä¼¼åº¦è¨ˆç®—é–‹å§‹...")
    results = []
    
    # ãƒãƒƒãƒã§å–å¾—ã—ãŸåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨æ¯”è¼ƒ
    for i, (target_emb, file_idx) in enumerate(zip(target_embeddings, valid_indices)):
        try:
            file_info = valid_file_info_list[file_idx]
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ï¼ˆé«˜é€ŸåŒ–ï¼‰
            similarity_score = float(np.dot(query_emb, target_emb))
            
            # çµæœã‚’è¿½åŠ 
            results.append({
                'filename': os.path.basename(file_info['filename']),
                'original_filename': file_info['original_name'],
                'image_path': "/" + file_info['filename'],
                'best_similarity': similarity_score,
                'best_model': MODEL_CONFIG['name'],
                'model_results': {
                    'buffalo_l': {
                        'model_name': MODEL_CONFIG['name'],
                        'similarity': similarity_score,
                        'confidence': min(similarity_score * 1.2, 1.0),
                        'is_same': similarity_score > 0.45
                    }
                },
                'is_match': similarity_score > 0.45
            })
            
            # é€²æ—è¡¨ç¤º
            if (i + 1) % 500 == 0 or i == len(target_embeddings) - 1:
                progress = (i + 1) / len(target_embeddings) * 100
                print(f"ğŸ“ˆ é¡ä¼¼åº¦è¨ˆç®—: {i + 1}/{len(target_embeddings)} ({progress:.1f}%)")
            
        except Exception as e:
            print(f"âŒ é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼ [{i}]: {e}")
            file_info = valid_file_info_list[file_idx] if file_idx < len(valid_file_info_list) else {'filename': 'unknown', 'original_name': 'unknown'}
            results.append({
                'filename': os.path.basename(file_info.get('filename', 'unknown')),
                'original_filename': file_info.get('original_name', 'unknown'),
                'image_path': None,
                'best_similarity': 0.0,
                'best_model': 'N/A',
                'model_results': {},
                'is_match': False,
                'error': str(e)
            })
    
    # å‡¦ç†ã§ããªã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒãƒƒãƒå‡¦ç†ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’è¿½åŠ 
    processed_indices = set(valid_indices)
    for idx, file_info in enumerate(valid_file_info_list):
        if idx not in processed_indices:
            results.append({
                'filename': os.path.basename(file_info['filename']),
                'original_filename': file_info['original_name'],
                'image_path': "/" + file_info['filename'],
                'best_similarity': 0.0,
                'best_model': 'N/A',
                'model_results': {},
                'is_match': False,
                'error': 'ãƒãƒƒãƒå‡¦ç†ã§ã‚¹ã‚­ãƒƒãƒ—'
            })
    
    print(f"âœ… Buffalo_l ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(results)}ä»¶ã®çµæœ")
    
    # é¡ä¼¼åº¦ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x['best_similarity'], reverse=True)
    
    # é †ä½ã‚’è¿½åŠ 
    for idx, result in enumerate(results):
        result['rank'] = idx + 1
    
    top10_similarities = [f"{r['best_similarity']:.3f}" for r in results[:10]]
    print(f"ğŸ† çµæœã‚½ãƒ¼ãƒˆå®Œäº†: ä¸Šä½10ä»¶ã®é¡ä¼¼åº¦ {top10_similarities}")
    
    processing_time = time.time() - start_time
    print(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’ ({len(results)/processing_time:.1f}ãƒ•ã‚¡ã‚¤ãƒ«/ç§’)")
    
    return results


async def _execute_comparison_no_batch(query_embedding, valid_file_info_list, start_time):
    """ãƒãƒƒãƒå‡¦ç†ãªã—ã®æ¯”è¼ƒå‡¦ç†ï¼ˆé€Ÿåº¦æ¯”è¼ƒç”¨ï¼‰"""
    total_files = len(valid_file_info_list)
    
    print(f"ğŸŒ éãƒãƒƒãƒå‡¦ç†æ¯”è¼ƒé–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤é †æ¬¡å‡¦ç†ï¼‰")
    
    # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    query_emb = query_embedding.get('embedding')
    if query_emb is None:
        print("âŒ ã‚¯ã‚¨ãƒªç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    
    print(f"ğŸ”„ é †æ¬¡å‡¦ç†ã«ã‚ˆã‚‹é¡ä¼¼åº¦è¨ˆç®—é–‹å§‹...")
    results = []
    
    # 1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤é †æ¬¡å‡¦ç†
    for i, file_info in enumerate(valid_file_info_list):
        try:
            # 1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
            target_emb = get_embedding_single(file_info['filename'], use_detection=True)
            
            if target_emb is None:
                # å‡¦ç†å¤±æ•—ã®å ´åˆ
                results.append({
                    'filename': os.path.basename(file_info['filename']),
                    'original_filename': file_info['original_name'],
                    'image_path': "/" + file_info['filename'],
                    'best_similarity': 0.0,
                    'best_model': 'N/A',
                    'model_results': {},
                    'is_match': False,
                    'error': 'åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡ºå¤±æ•—'
                })
                continue
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
            similarity_score = float(np.dot(query_emb, target_emb))
            
            # çµæœã‚’è¿½åŠ 
            results.append({
                'filename': os.path.basename(file_info['filename']),
                'original_filename': file_info['original_name'],
                'image_path': "/" + file_info['filename'],
                'best_similarity': similarity_score,
                'best_model': MODEL_CONFIG['name'],
                'model_results': {
                    'buffalo_l': {
                        'model_name': MODEL_CONFIG['name'],
                        'similarity': similarity_score,
                        'confidence': min(similarity_score * 1.2, 1.0),
                        'is_same': similarity_score > 0.45
                    }
                },
                'is_match': similarity_score > 0.45
            })
            
            # é€²æ—è¡¨ç¤ºï¼ˆéãƒãƒƒãƒå‡¦ç†ã¯é…ã„ã®ã§ã‚ˆã‚Šé »ç¹ã«è¡¨ç¤ºï¼‰
            if (i + 1) % 50 == 0 or i == total_files - 1:
                progress = (i + 1) / total_files * 100
                elapsed_time = time.time() - start_time
                estimated_total = elapsed_time * total_files / (i + 1) if i > 0 else 0
                remaining_time = estimated_total - elapsed_time
                print(f"ğŸ“ˆ éãƒãƒƒãƒå‡¦ç†: {i + 1}/{total_files} ({progress:.1f}%) - æ®‹ã‚Šæ™‚é–“: {remaining_time:.1f}ç§’")
            
        except Exception as e:
            print(f"âŒ éãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ [{i}]: {e}")
            results.append({
                'filename': os.path.basename(file_info.get('filename', 'unknown')),
                'original_filename': file_info.get('original_name', 'unknown'),
                'image_path': "/" + file_info['filename'] if file_info.get('filename') else None,
                'best_similarity': 0.0,
                'best_model': 'N/A',
                'model_results': {},
                'is_match': False,
                'error': str(e)
            })
    
    print(f"âœ… éãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(results)}ä»¶ã®çµæœ")
    
    # é¡ä¼¼åº¦ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x['best_similarity'], reverse=True)
    
    # é †ä½ã‚’è¿½åŠ 
    for idx, result in enumerate(results):
        result['rank'] = idx + 1
    
    top10_similarities = [f"{r['best_similarity']:.3f}" for r in results[:10]]
    print(f"ğŸ† éãƒãƒƒãƒçµæœã‚½ãƒ¼ãƒˆå®Œäº†: ä¸Šä½10ä»¶ã®é¡ä¼¼åº¦ {top10_similarities}")
    
    processing_time = time.time() - start_time
    print(f"â±ï¸ éãƒãƒƒãƒç·å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’ ({len(results)/processing_time:.1f}ãƒ•ã‚¡ã‚¤ãƒ«/ç§’)")
    
    return results

async def _execute_comparison(query_embeddings, valid_file_info_list, selected_models, use_multiprocessing, batch_size, max_workers, memory_cleanup_interval, start_time, query_filename):
    """ãƒãƒƒãƒå‡¦ç†ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿæ¯”è¼ƒå‡¦ç†"""
    total_files = len(valid_file_info_list)
    
    print(f"ğŸš€ ãƒãƒƒãƒå‡¦ç†æ¯”è¼ƒé–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # ãƒãƒƒãƒå‡¦ç†ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸€æ‹¬å–å¾—
    target_file_paths = [file_info['filename'] for file_info in valid_file_info_list]
    
    print(f"ğŸ“Š ãƒãƒƒãƒç‰¹å¾´é‡æŠ½å‡ºé–‹å§‹... (ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size})")
    target_embeddings, valid_indices = get_embedding_batch(
        target_file_paths, 
        model_key='buffalo_l', 
        use_detection=True,
        batch_size=batch_size  # æ˜ç¤ºçš„ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æŒ‡å®š
    )
    
    if not target_embeddings:
        print("âŒ ãƒãƒƒãƒç‰¹å¾´é‡æŠ½å‡ºã«å¤±æ•—")
        return []
    
    print(f"âœ… ãƒãƒƒãƒç‰¹å¾´é‡æŠ½å‡ºå®Œäº†: {len(target_embeddings)}å€‹ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«")
    
    # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    query_emb = query_embeddings.get('buffalo_l', {}).get('embedding')
    if query_emb is None:
        print("âŒ ã‚¯ã‚¨ãƒªç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    
    print(f"ğŸ”„ é¡ä¼¼åº¦è¨ˆç®—é–‹å§‹...")
    results = []
    
    # ãƒãƒƒãƒã§å–å¾—ã—ãŸåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨æ¯”è¼ƒ
    for i, (target_emb, file_idx) in enumerate(zip(target_embeddings, valid_indices)):
        try:
            file_info = valid_file_info_list[file_idx]
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ï¼ˆé«˜é€ŸåŒ–ï¼‰
            similarity_score = float(np.dot(query_emb, target_emb))
            
            # çµæœã‚’è¿½åŠ 
            results.append({
                'filename': os.path.basename(file_info['filename']),
                'original_filename': file_info['original_name'],
                'image_path': "/" + file_info['filename'],
                'best_similarity': similarity_score,
                'best_model': MODEL_CONFIG['name'],
                'model_results': {
                    'buffalo_l': {
                        'model_name': MODEL_CONFIG['name'],
                        'similarity': similarity_score,
                        'confidence': min(similarity_score * 1.2, 1.0),
                        'is_same': similarity_score > 0.45
                    }
                },
                'is_match': similarity_score > 0.45
            })
            
            # é€²æ—è¡¨ç¤º
            if (i + 1) % 500 == 0 or i == len(target_embeddings) - 1:
                progress = (i + 1) / len(target_embeddings) * 100
                print(f"ğŸ“ˆ é¡ä¼¼åº¦è¨ˆç®—: {i + 1}/{len(target_embeddings)} ({progress:.1f}%)")
            
        except Exception as e:
            print(f"âŒ é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼ [{i}]: {e}")
            file_info = valid_file_info_list[file_idx] if file_idx < len(valid_file_info_list) else {'filename': 'unknown', 'original_name': 'unknown'}
            results.append({
                'filename': os.path.basename(file_info.get('filename', 'unknown')),
                'original_filename': file_info.get('original_name', 'unknown'),
                'image_path': None,
                'best_similarity': 0.0,
                'best_model': 'N/A',
                'model_results': {},
                'is_match': False,
                'error': str(e)
            })
    
    # å‡¦ç†ã§ããªã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒãƒƒãƒå‡¦ç†ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’è¿½åŠ 
    processed_indices = set(valid_indices)
    for idx, file_info in enumerate(valid_file_info_list):
        if idx not in processed_indices:
            results.append({
                'filename': os.path.basename(file_info['filename']),
                'original_filename': file_info['original_name'],
                'image_path': "/" + file_info['filename'],
                'best_similarity': 0.0,
                'best_model': 'N/A',
                'model_results': {},
                'is_match': False,
                'error': 'ãƒãƒƒãƒå‡¦ç†ã§ã‚¹ã‚­ãƒƒãƒ—'
            })
    
    print(f"âœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(results)}ä»¶ã®çµæœ")
    
    # é¡ä¼¼åº¦ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x['best_similarity'], reverse=True)
    
    # é †ä½ã‚’è¿½åŠ 
    for idx, result in enumerate(results):
        result['rank'] = idx + 1
    
    top10_similarities = [f"{r['best_similarity']:.3f}" for r in results[:10]]
    print(f"ğŸ† çµæœã‚½ãƒ¼ãƒˆå®Œäº†: ä¸Šä½10ä»¶ã®é¡ä¼¼åº¦ {top10_similarities}")
    
    processing_time = time.time() - start_time
    print(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’ ({len(results)/processing_time:.1f}ãƒ•ã‚¡ã‚¤ãƒ«/ç§’)")
    
    return results

def _format_comparison_results(results, query_image, total_files, valid_file_info_list, start_time, is_chunk=False):
    """çµæœã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    total_processing_time = time.time() - start_time
    files_per_second = len(results) / total_processing_time if total_processing_time > 0 else 0
    
    return JSONResponse(content={
        'query_image': query_image.filename,
        'total_comparisons': total_files,
        'successful_comparisons': len(results),
        'matches_found': len([r for r in results if r['is_match']]),
        'results': results,  # å…¨ä»¶è¡¨ç¤º
        'total_results': len(results),
        'showing_top': len(results),
        'is_chunk': is_chunk,
        'processing_summary': {
            'model': MODEL_CONFIG['name'],
            'model_description': MODEL_CONFIG['description'],
            'threshold_used': 0.45,
            'total_processing_time': total_processing_time * 1000,
            'files_per_second': files_per_second,
            'optimization_level': 'high' if total_files > 1000 else 'standard'
        }
    })

@app.post("/compare_folder_benchmark")
async def compare_folder_benchmark(
    query_image: UploadFile = File(...),
    folder_images: List[UploadFile] = File(...),
    use_batch: bool = Form(True)  # ãƒãƒƒãƒå‡¦ç†ã®æœ‰ç„¡ã‚’é¸æŠ
):
    """é€Ÿåº¦æ¯”è¼ƒç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - ãƒãƒƒãƒå‡¦ç†ã‚ã‚Š/ãªã—ã®æ€§èƒ½æ¯”è¼ƒ"""
    start_time = time.time()
    total_files = len(folder_images)
    
    print(f"ğŸ é€Ÿåº¦æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«, ãƒãƒƒãƒå‡¦ç†={'æœ‰åŠ¹' if use_batch else 'ç„¡åŠ¹'}")
    
    try:
        # ã‚¯ã‚¨ãƒªç”»åƒã‚’ä¿å­˜
        os.makedirs("static/temp", exist_ok=True)
        query_filename = f"static/temp/query_{uuid.uuid4().hex}_{query_image.filename}"
        with open(query_filename, "wb") as buffer:
            shutil.copyfileobj(query_image.file, buffer)
        
        # ã‚¯ã‚¨ãƒªç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
        query_embedding = get_embedding_buffalo(query_filename, use_detection=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å‡¦ç†
        file_info_list = await _save_files_individually(folder_images)
        valid_file_info_list = [f for f in file_info_list if not f.get('error') and f.get('filename')]
        
        preprocessing_time = time.time() - start_time
        comparison_start_time = time.time()
        
        if use_batch:
            # ãƒãƒƒãƒå‡¦ç†ç‰ˆ
            optimal_batch_size = calculate_optimal_batch_size(len(valid_file_info_list))
            print(f"ğŸš€ ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ (æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º: {optimal_batch_size})")
            results = await _execute_comparison_buffalo(query_embedding, valid_file_info_list, optimal_batch_size, comparison_start_time)
        else:
            # éãƒãƒƒãƒå‡¦ç†ç‰ˆ
            print(f"ğŸŒ éãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ (1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤é †æ¬¡å‡¦ç†)")
            results = await _execute_comparison_no_batch(query_embedding, valid_file_info_list, comparison_start_time)
        
        comparison_time = time.time() - comparison_start_time
        total_time = time.time() - start_time
        
        # é€Ÿåº¦çµ±è¨ˆã®è¨ˆç®—
        files_per_second = len(valid_file_info_list) / comparison_time if comparison_time > 0 else 0
        
        # çµæœã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: resultså‹={type(results)}, é•·ã•={len(results) if results else 'None'}")
        if results:
            print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®çµæœ={results[0] if len(results) > 0 else 'Empty'}")
        
        # top_matchesã®ãƒ‡ãƒãƒƒã‚°
        top_matches = results[:10] if results else []
        print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: top_matcheså‹={type(top_matches)}, é•·ã•={len(top_matches)}")
        
        return JSONResponse(content={
            'benchmark_mode': 'batch' if use_batch else 'no_batch',
            'query_image': query_image.filename,
            'total_files': total_files,
            'processed_files': len(valid_file_info_list),
            'matches_found': len([r for r in results if r['is_match']]) if results else 0,
            'results': results[:100] if results else [],  # ä¸Šä½100ä»¶ã®ã¿è¡¨ç¤º
            'total_results': len(results) if results else 0,
            'performance_metrics': {
                'preprocessing_time_ms': preprocessing_time * 1000,
                'comparison_time_ms': comparison_time * 1000,
                'total_time_ms': total_time * 1000,
                'files_per_second': files_per_second,
                'avg_time_per_file_ms': (comparison_time / len(valid_file_info_list) * 1000) if valid_file_info_list else 0,
                'processing_method': 'ãƒãƒƒãƒå‡¦ç†' if use_batch else 'é †æ¬¡å‡¦ç†ï¼ˆéãƒãƒƒãƒï¼‰',
                'efficiency_score': files_per_second * len(valid_file_info_list)  # ç·åˆåŠ¹ç‡ã‚¹ã‚³ã‚¢
            },
            'top_matches': top_matches
        })
        
    except Exception as e:
        import traceback
        error_traceback = traceback.format_exc()
        print(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
        print(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼è©³ç´°:\n{error_traceback}")
        
        return JSONResponse(
            status_code=500,
            content={
                'error': f'ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}',
                'benchmark_mode': 'batch' if use_batch else 'no_batch',
                'total_files': total_files,
                'processing_status': 'failed'
            }
        )

@app.post("/compare_folder")
async def compare_folder(
    request: Request,
    query_image: UploadFile = File(...),
    folder_images: List[UploadFile] = File(...)
):
    """1å¯¾Næ¯”è¼ƒã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - 1ã¤ã®ç”»åƒã¨ãƒ•ã‚©ãƒ«ãƒ€ã®å…¨ç”»åƒã‚’æ¯”è¼ƒï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™è§£é™¤ç‰ˆï¼‰"""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™ã®äº‹å‰ãƒã‚§ãƒƒã‚¯ã¨ãƒã‚¤ãƒ‘ã‚¹
    file_count = len(folder_images) if folder_images else 0
    print(f"ğŸ“‹ å—ä¿¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}")
    
    if file_count > 1000:
        print(f"ğŸš¨ å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {file_count}ãƒ•ã‚¡ã‚¤ãƒ« - åˆ¶é™è§£é™¤ãƒ¢ãƒ¼ãƒ‰ã§å‡¦ç†")
        
        # å®Ÿè¡Œæ™‚åˆ¶é™ã®å¼·åˆ¶è§£é™¤
        try:
            import starlette.formparsers as fp
            if hasattr(fp, 'MAX_FORM_FILES'):
                original = fp.MAX_FORM_FILES
                fp.MAX_FORM_FILES = 100000  # æ¥µé™ã¾ã§å¢—åŠ 
                print(f"ğŸ”§ å®Ÿè¡Œæ™‚åˆ¶é™å¼·åŒ–: MAX_FORM_FILES {original} â†’ 100000")
                
            # ãã®ä»–ã®åˆ¶é™ã‚‚ç·©å’Œ
            if hasattr(fp, 'MAX_FORM_PART_SIZE'):
                fp.MAX_FORM_PART_SIZE = 200 * 1024 * 1024  # 200MB
                print(f"ğŸ”§ ãƒ‘ãƒ¼ãƒˆã‚µã‚¤ã‚ºåˆ¶é™å¼·åŒ–: 200MB")
                
        except Exception as e:
            print(f"âš ï¸ å®Ÿè¡Œæ™‚åˆ¶é™è§£é™¤ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"ğŸ¯ åˆ¶é™ç¢ºèªå®Œäº† - å‡¦ç†é–‹å§‹å¯èƒ½")
    start_time = time.time()
    
    try:
        # è©³ç´°ãƒªã‚¯ã‚¨ã‚¹ãƒˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        content_length = request.headers.get("content-length", "ä¸æ˜")
        content_type = request.headers.get("content-type", "ä¸æ˜")
        user_agent = request.headers.get("user-agent", "ä¸æ˜")
        
        print(f"ğŸŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆè©³ç´°:")
        print(f"   ğŸ“ Content-Length: {content_length}")
        print(f"   ğŸ“‹ Content-Type: {content_type}")
        print(f"   ğŸ–¥ï¸  User-Agent: {user_agent}")
        print(f"   ğŸ“ ã‚¯ã‚¨ãƒªç”»åƒ: {query_image.filename if query_image else 'None'}")
        print(f"   ğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€ç”»åƒæ•°: {len(folder_images) if folder_images else 0}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®æ¨å®š
        if folder_images and len(folder_images) > 0:
            sample_size = 0
            sample_count = min(5, len(folder_images))
            for i in range(sample_count):
                try:
                    content = await folder_images[i].read()
                    size = len(content)
                    sample_size += size
                    await folder_images[i].seek(0)  # ãƒã‚¤ãƒ³ã‚¿ã‚’æˆ»ã™
                    print(f"   ğŸ“ ã‚µãƒ³ãƒ—ãƒ«{i+1}: {folder_images[i].filename} ({size:,}ãƒã‚¤ãƒˆ)")
                except Exception as e:
                    print(f"   âŒ ã‚µãƒ³ãƒ—ãƒ«{i+1}èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
            
            if sample_count > 0:
                avg_size = sample_size / sample_count
                estimated_total = avg_size * len(folder_images)
                print(f"   ğŸ’¾ æ¨å®šç·ã‚µã‚¤ã‚º: {estimated_total / (1024*1024):.1f}MB")
                
                # å¤§é‡ãƒ‡ãƒ¼ã‚¿è­¦å‘Š
                if estimated_total > 500 * 1024 * 1024:  # 500MBä»¥ä¸Š
                    print(f"âš ï¸  å¤§é‡ãƒ‡ãƒ¼ã‚¿è­¦å‘Š: æ¨å®š{estimated_total / (1024*1024):.1f}MB")
        
    except Exception as e:
        print(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆè§£æã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
    print(f"ğŸ” ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡: query_image={query_image.filename if query_image else 'None'}")
    print(f"ğŸ” ãƒ•ã‚©ãƒ«ãƒ€ç”»åƒæ•°: {len(folder_images) if folder_images else 0}")
    
    # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚·ãƒ³ãƒ—ãƒ«ãªé †æ¬¡å‡¦ç†ã§å®Ÿè¡Œ
    print(f"ğŸ“‹ ã‚·ãƒ³ãƒ—ãƒ«å‡¦ç†å®Ÿè¡Œ: {len(folder_images)}ãƒ•ã‚¡ã‚¤ãƒ«")
    return await compare_folder_internal(query_image, folder_images)
