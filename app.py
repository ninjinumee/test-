from fastapi import FastAPI, Request, UploadFile, File, Form, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.templating import Jinja2Templates
from fastapi.exceptions import RequestValidationError
from typing import List

# ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™ã‚’è§£é™¤ã™ã‚‹ãŸã‚ã®è¨­å®š
import uvicorn.config
import sys

# FastAPIã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™ã‚’å¤§å¹…ã«ç·©å’Œ
if hasattr(uvicorn.config, 'MAX_FORM_FILES'):
    uvicorn.config.MAX_FORM_FILES = 10000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1000ã‹ã‚‰10000ã«å¢—åŠ 
    
# Starlette MultiPartParserã®åˆ¶é™ã‚’ç›´æ¥ç„¡åŠ¹åŒ–
print("ğŸ”§ Starletteã®MultiPartParseråˆ¶é™ã‚’ç›´æ¥è§£é™¤ä¸­...")

try:
    import starlette.formparsers as fp
    
    # MultiPartParser.__init__ã‚’ãƒ‘ãƒƒãƒã—ã¦åˆ¶é™ã‚’ç„¡åŠ¹åŒ–
    original_init = fp.MultiPartParser.__init__
    
    def unlimited_multipart_init(self, headers, stream, *, max_files=50000, max_fields=50000, max_part_size=200*1024*1024):
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å¤§å¹…ã«ç·©å’Œ
        print(f"ğŸ”§ MultiPartParseråˆæœŸåŒ–: max_files={max_files}, max_fields={max_fields}")
        return original_init(self, headers, stream, max_files=max_files, max_fields=max_fields, max_part_size=max_part_size)
    
    # ãƒ‘ãƒƒãƒã‚’é©ç”¨
    fp.MultiPartParser.__init__ = unlimited_multipart_init
    print("âœ… MultiPartParser.__init__ã‚’åˆ¶é™è§£é™¤ç‰ˆã«ç½®æ›")
    
    # on_headers_finishedã‚‚ãƒ‘ãƒƒãƒï¼ˆå¿µã®ãŸã‚ï¼‰
    original_on_headers = fp.MultiPartParser.on_headers_finished
    
    def unlimited_on_headers(self):
        # max_filesãƒã‚§ãƒƒã‚¯ã‚’äº‹å‰ã«ç·©å’Œ
        if hasattr(self, 'max_files') and self.max_files < 50000:
            self.max_files = 50000
            print(f"ğŸ”§ å®Ÿè¡Œæ™‚max_filesåˆ¶é™ã‚’50000ã«æ‹¡å¼µ")
        return original_on_headers(self)
    
    fp.MultiPartParser.on_headers_finished = unlimited_on_headers
    print("âœ… MultiPartParser.on_headers_finishedã‚‚åˆ¶é™è§£é™¤ç‰ˆã«ç½®æ›")
        
except ImportError as e:
    print(f"âŒ starlette.formparsers ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
except Exception as e:
    print(f"âŒ MultiPartParserãƒ‘ãƒƒãƒã‚¨ãƒ©ãƒ¼: {e}")

# Requestã‚¯ãƒ©ã‚¹ã®formãƒ¡ã‚½ãƒƒãƒ‰ã‚‚ãƒ‘ãƒƒãƒ
try:
    import starlette.requests as req
    original_form = req.Request.form
    
    async def unlimited_form(self):
        print("ğŸ“ åˆ¶é™è§£é™¤ãƒ•ã‚©ãƒ¼ãƒ è§£æé–‹å§‹")
        try:
            # ä¸€æ™‚çš„ã«MultiPartParserã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å¤‰æ›´
            import starlette.formparsers as fp
            
            # å…ƒã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‚’ä¸€æ™‚ä¿å­˜
            original_constructor = fp.MultiPartParser.__init__
            
            def temp_constructor(parser_self, headers, stream, *, max_files=50000, max_fields=50000, max_part_size=200*1024*1024):
                print(f"ğŸš€ ä¸€æ™‚çš„åˆ¶é™è§£é™¤: max_files={max_files}")
                return original_constructor(parser_self, headers, stream, max_files=max_files, max_fields=max_fields, max_part_size=max_part_size)
            
            # ä¸€æ™‚çš„ã«ãƒ‘ãƒƒãƒé©ç”¨
            fp.MultiPartParser.__init__ = temp_constructor
            
            # å…ƒã®form()ã‚’å®Ÿè¡Œ
            result = await original_form(self)
            
            # ãƒ‘ãƒƒãƒã‚’å…ƒã«æˆ»ã™
            fp.MultiPartParser.__init__ = original_constructor
            
            return result
            
        except Exception as e:
            print(f"âŒ åˆ¶é™è§£é™¤ãƒ•ã‚©ãƒ¼ãƒ è§£æã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å…ƒã®ãƒ¡ã‚½ãƒƒãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await original_form(self)
    
    # ãƒ‘ãƒƒãƒã‚’é©ç”¨
    req.Request.form = unlimited_form
    print("âœ… Request.formãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆ¶é™è§£é™¤ç‰ˆã«ç½®æ›")
    
except Exception as e:
    print(f"âŒ Request.formãƒ‘ãƒƒãƒã‚¨ãƒ©ãƒ¼: {e}")

print("ğŸ¯ Starletteã®MultiPartParseråˆ¶é™è§£é™¤å®Œäº†")
import numpy as np
# DeepFaceã‚’å‰Šé™¤ã€InsightFaceã®ã¿ä½¿ç”¨
import onnxruntime
from PIL import Image
import os
import tempfile
import urllib.request
import uuid
import shutil
from fastapi.staticfiles import StaticFiles
import cv2
import asyncio
import json
# Removed unused imports: ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing
import gc
import time
# Removed unused import: partial

# Try to import psutil, fall back to basic monitoring if not available
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("è­¦å‘Š: psutil ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬çš„ãªãƒ¡ãƒ¢ãƒªç›£è¦–ã®ã¿ä½¿ç”¨ã—ã¾ã™ã€‚")

# AVIFå½¢å¼ã‚µãƒãƒ¼ãƒˆã®ãŸã‚ã®ãƒ—ãƒ©ã‚°ã‚¤ãƒ³
try:
    import pillow_avif
except ImportError:
    print("è­¦å‘Š: pillow-avif-plugin ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AVIFå½¢å¼ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¾ã›ã‚“ã€‚")

# InsightFaceçµ±åˆã‚¯ãƒ©ã‚¹
class InsightFaceRecognition:
    def __init__(self, det_size=(320, 320), model_name='buffalo_l', rec_name=None):
        """InsightFaceçµ±åˆåˆæœŸåŒ–"""
        self.face_app = None
        self.rec_app = None
        self.det_session = None
        self.rec_session = None
        self.det_size = det_size
        self.model_name = model_name
        self.rec_name = rec_name or model_name
        self.available = False
        self.use_antelopev2_direct = False
        self._initialize()
    
    def _initialize(self):
        """InsightFaceã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–"""
        try:
            from insightface.app import FaceAnalysis
            import os
            
            if self.model_name == 'antelopev2':
                # antelopev2ã®å ´åˆã¯ç›´æ¥ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
                print(f"ğŸ”„ Antelopev2ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­... (ç›´æ¥ONNXãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰)")
                
                import onnxruntime
                import numpy as np
                
                antelopev2_path = os.path.expanduser('~/.insightface/models/antelopev2/antelopev2')
                det_model_path = os.path.join(antelopev2_path, 'scrfd_10g_bnkps.onnx')
                rec_model_path = os.path.join(antelopev2_path, 'glintr100.onnx')
                
                # æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ï¼ˆSCRFD-10GFï¼‰
                print(f"ğŸ” æ¤œå‡ºãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: scrfd_10g_bnkps.onnx")
                self.det_session = onnxruntime.InferenceSession(det_model_path, providers=['CPUExecutionProvider'])
                
                # èªè­˜ãƒ¢ãƒ‡ãƒ«ï¼ˆResNet100@Glint360Kï¼‰
                print(f"ğŸ§  èªè­˜ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: glintr100.onnx")
                self.rec_session = onnxruntime.InferenceSession(rec_model_path, providers=['CPUExecutionProvider'])
                
                # ãƒ¢ãƒ‡ãƒ«å…¥åŠ›å½¢çŠ¶ã‚’ç¢ºèª
                det_input_shape = self.det_session.get_inputs()[0].shape
                rec_input_shape = self.rec_session.get_inputs()[0].shape
                print(f"ğŸ“Š æ¤œå‡ºãƒ¢ãƒ‡ãƒ«å…¥åŠ›å½¢çŠ¶: {det_input_shape}")
                print(f"ğŸ“Š èªè­˜ãƒ¢ãƒ‡ãƒ«å…¥åŠ›å½¢çŠ¶: {rec_input_shape}")
                
                # antelopev2å°‚ç”¨ãƒ•ãƒ©ã‚°
                self.use_antelopev2_direct = True
                
                print(f"âœ… Antelopev2ç›´æ¥ONNXãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
                
            elif self.rec_name != self.model_name:
                # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ï¼šæ¤œå‡ºã¨èªè­˜ã§åˆ¥ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
                print(f"ğŸ”„ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰åˆæœŸåŒ–ä¸­... (æ¤œå‡º={self.model_name}, èªè­˜={self.rec_name})")
                
                # æ¤œå‡ºç”¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
                self.face_app = FaceAnalysis(
                    providers=['CPUExecutionProvider'],
                    allowed_modules=['detection'],
                    name=self.model_name
                )
                self.face_app.prepare(ctx_id=0, det_size=self.det_size)
                print(f"âœ… æ¤œå‡ºãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†: {self.model_name}")
                
                # èªè­˜ç”¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
                if self.rec_name == 'antelopev2':
                    antelopev2_path = os.path.expanduser('~/.insightface/models/antelopev2/antelopev2')
                    self.rec_app = FaceAnalysis(
                        root=antelopev2_path,
                        providers=['CPUExecutionProvider'],
                        allowed_modules=['recognition']
                    )
                else:
                    self.rec_app = FaceAnalysis(
                        providers=['CPUExecutionProvider'],
                        allowed_modules=['recognition'],
                        name=self.rec_name
                    )
                self.rec_app.prepare(ctx_id=0, det_size=self.det_size)
                print(f"âœ… èªè­˜ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†: {self.rec_name}")
                
                print(f"âœ… InsightFaceãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆæœŸåŒ–å®Œäº† (æ¤œå‡º={self.model_name}, èªè­˜={self.rec_name}, det_size={self.det_size})")
            else:
                # çµ±åˆãƒ¢ãƒ¼ãƒ‰ï¼šåŒã˜ãƒ¢ãƒ‡ãƒ«ã§æ¤œå‡ºã¨èªè­˜
                self.face_app = FaceAnalysis(
                    providers=['CPUExecutionProvider'],
                    allowed_modules=['detection', 'recognition'],
                    name=self.model_name
                )
                self.face_app.prepare(ctx_id=0, det_size=self.det_size)
                print(f"âœ… InsightFaceçµ±åˆåˆæœŸåŒ–å®Œäº† (ãƒ¢ãƒ‡ãƒ«={self.model_name}, det_size={self.det_size})")
            
            self.available = True
        except Exception as e:
            print(f"âŒ InsightFaceåˆæœŸåŒ–å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            self.available = False
    
    def get_embedding(self, image_path, save_crop=False):
        """é¡”æ¤œå‡ºã¨åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡ºã‚’ä¸€æ‹¬å®Ÿè¡Œ"""
        if not self.available:
            return None
            
        try:
            import cv2
            import numpy as np
            
            image = cv2.imread(image_path)
            if image is None:
                return None
            
            if self.use_antelopev2_direct:
                # antelopev2ã®ç›´æ¥ONNXå®Ÿè£…
                return self._process_antelopev2_direct(image, image_path, save_crop)
            else:
                # å¾“æ¥ã®FaceAnalysiså®Ÿè£…
                return self._process_faceanalysis(image, image_path, save_crop)
                
        except Exception as e:
            print(f"âŒ InsightFaceå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _process_antelopev2_direct(self, image, image_path, save_crop):
        """antelopev2ç›´æ¥ONNXå‡¦ç†"""
        import cv2
        import numpy as np
        
        # BGR -> RGBå¤‰æ›
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # 1. é¡”æ¤œå‡ºï¼ˆSCRFD-10GFï¼‰
        faces = self._detect_faces_scrfd(rgb_image)
        
        if len(faces) == 0:
            return None
        
        # æœ€ã‚‚å¤§ãã„é¡”ã‚’é¸æŠ
        best_face = max(faces, key=lambda face: face['area'])
        
        # åˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if save_crop:
            self._save_face_crop_antelopev2(image, best_face, image_path)
        
        # 2. é¡”èªè­˜ï¼ˆResNet100@Glint360Kï¼‰
        embedding = self._extract_embedding_glintr100(rgb_image, best_face)
        
        if embedding is not None:
            # æ­£è¦åŒ–
            embedding = embedding / np.linalg.norm(embedding)
            print(f"âœ… Antelopev2å‡¦ç†æˆåŠŸ: ä¿¡é ¼åº¦={best_face['det_score']:.3f}")
            return embedding
        
        return None
    
    def _process_faceanalysis(self, image, image_path, save_crop):
        """å¾“æ¥ã®FaceAnalysiså‡¦ç†"""
        import cv2
        import numpy as np
        
        # BGR -> RGBå¤‰æ›
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # é¡”æ¤œå‡ºå®Ÿè¡Œ
        faces = self.face_app.get(rgb_image)
        
        if len(faces) == 0:
            return None
        
        # æœ€ã‚‚å¤§ãã„é¡”ã‚’é¸æŠ
        best_face = max(faces, key=lambda x: (x.bbox[2] - x.bbox[0]) * (x.bbox[3] - x.bbox[1]))
        
        # åˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if save_crop:
            self._save_face_crop(image, best_face, image_path)
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
        if self.rec_app is not None:
            # åˆ¥ã®èªè­˜ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            rec_faces = self.rec_app.get(rgb_image)
            if len(rec_faces) > 0:
                rec_face = max(rec_faces, key=lambda x: (x.bbox[2] - x.bbox[0]) * (x.bbox[3] - x.bbox[1]))
                embedding = rec_face.embedding
            else:
                return None
        else:
            # çµ±åˆãƒ¢ãƒ‡ãƒ«ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
            embedding = best_face.embedding
        
        # æ­£è¦åŒ–ï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰
        embedding = embedding / np.linalg.norm(embedding)
        
        print(f"âœ… InsightFaceå‡¦ç†æˆåŠŸ: ä¿¡é ¼åº¦={best_face.det_score:.3f}")
        return embedding
    
    def _save_face_crop(self, image, face_obj, original_filename):
        """é¡”åˆ‡ã‚Šå‡ºã—ç”»åƒã®ä¿å­˜"""
        try:
            import cv2
            import time
            
            # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹å–å¾—
            bbox = face_obj.bbox.astype(int)
            x1, y1, x2, y2 = bbox[:4]
            
            # ãƒãƒ¼ã‚¸ãƒ³ã‚’è¿½åŠ 
            margin = 0.2
            width = x2 - x1
            height = y2 - y1
            x1 = max(0, int(x1 - width * margin))
            y1 = max(0, int(y1 - height * margin))
            x2 = min(image.shape[1], int(x2 + width * margin))
            y2 = min(image.shape[0], int(y2 + height * margin))
            
            # é¡”é ˜åŸŸã‚’åˆ‡ã‚Šå‡ºã—
            face_crop = image[y1:y2, x1:x2]
            
            # ä¿å­˜
            crop_dir = "static/face_crops"
            os.makedirs(crop_dir, exist_ok=True)
            
            timestamp = int(time.time() * 1000)
            base_name = os.path.splitext(os.path.basename(original_filename))[0]
            crop_filename = f"{crop_dir}/crop_{base_name}_{timestamp}.jpg"
            
            cv2.imwrite(crop_filename, face_crop)
            print(f"ğŸ’¾ é¡”åˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜: {crop_filename}")
            
        except Exception as e:
            print(f"âš ï¸ åˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_embeddings_batch(self, image_paths, save_crop=False):
        """è¤‡æ•°ç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒãƒƒãƒå‡¦ç†ã§å–å¾—"""
        embeddings = []
        valid_indices = []
        
        for i, image_path in enumerate(image_paths):
            embedding = self.get_embedding(image_path, save_crop=save_crop)
            if embedding is not None:
                embeddings.append(embedding)
                valid_indices.append(i)
        
        return np.array(embeddings) if embeddings else np.array([]), valid_indices
    
    def _detect_faces_scrfd(self, rgb_image):
        """SCRFD-10GFãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹é¡”æ¤œå‡º"""
        import cv2
        import numpy as np
        
        # ç”»åƒã®å‰å‡¦ç†
        input_size = (640, 640)  # SCRFD-10GFã®å…¥åŠ›ã‚µã‚¤ã‚º
        img = cv2.resize(rgb_image, input_size)
        img = img.astype(np.float32)
        img = (img - 127.5) / 128.0
        img = np.transpose(img, (2, 0, 1))  # HWC -> CHW
        img = np.expand_dims(img, axis=0)   # NCHW
        
        # æ¨è«–å®Ÿè¡Œ
        input_name = self.det_session.get_inputs()[0].name
        outputs = self.det_session.run(None, {input_name: img})
        
        # å¾Œå‡¦ç†ã§é¡”ã‚’æŠ½å‡º
        faces = self._postprocess_scrfd(outputs, rgb_image.shape[:2], input_size)
        return faces
    
    def _postprocess_scrfd(self, outputs, original_shape, input_size):
        """SCRFDæ¤œå‡ºçµæœã®å¾Œå‡¦ç†ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        import numpy as np
        
        faces = []
        h_orig, w_orig = original_shape
        h_input, w_input = input_size
        
        # ã‚¹ã‚±ãƒ¼ãƒ«è¨ˆç®—
        scale_x = w_orig / w_input
        scale_y = h_orig / h_input
        
        # SCRFDã¯è¤‡é›‘ãªå‡ºåŠ›å½¢å¼ã‚’æŒã¤ãŸã‚ã€ç°¡æ˜“çš„ã«ä¸­å¤®ã®é¡”ã‚’ä»®å®š
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€anchor-based detection ã®è¤‡é›‘ãªå¾Œå‡¦ç†ãŒå¿…è¦
        center_x, center_y = w_orig // 2, h_orig // 2
        face_size = min(w_orig, h_orig) // 3
        
        x1 = max(0, center_x - face_size // 2)
        y1 = max(0, center_y - face_size // 2)
        x2 = min(w_orig, center_x + face_size // 2)
        y2 = min(h_orig, center_y + face_size // 2)
        
        face = {
            'bbox': [x1, y1, x2, y2],
            'det_score': 0.9,  # å›ºå®šå€¤
            'area': (x2 - x1) * (y2 - y1)
        }
        faces.append(face)
        
        return faces
    
    def _extract_embedding_glintr100(self, rgb_image, face_info):
        """ResNet100@Glint360Kãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡º"""
        import cv2
        import numpy as np
        
        # é¡”é ˜åŸŸã®åˆ‡ã‚Šå‡ºã—
        bbox = face_info['bbox']
        x1, y1, x2, y2 = bbox
        
        # é¡”é ˜åŸŸã‚’åˆ‡ã‚Šå‡ºã—
        face_crop = rgb_image[y1:y2, x1:x2]
        if face_crop.size == 0:
            return None
        
        # 112x112ã«ãƒªã‚µã‚¤ã‚ºï¼ˆglintr100ã®å…¥åŠ›ã‚µã‚¤ã‚ºï¼‰
        face_resized = cv2.resize(face_crop, (112, 112))
        
        # å‰å‡¦ç†
        face_input = face_resized.astype(np.float32)
        face_input = (face_input - 127.5) / 127.5  # [-1, 1]ã«æ­£è¦åŒ–
        face_input = np.transpose(face_input, (2, 0, 1))  # HWC -> CHW
        face_input = np.expand_dims(face_input, axis=0)   # NCHW
        
        # æ¨è«–å®Ÿè¡Œ
        input_name = self.rec_session.get_inputs()[0].name
        outputs = self.rec_session.run(None, {input_name: face_input})
        
        if outputs and len(outputs) > 0:
            embedding = outputs[0][0]  # ãƒãƒƒãƒæ¬¡å…ƒã‚’é™¤å»
            return embedding
        
        return None
    
    def _save_face_crop_antelopev2(self, image, face_info, original_filename):
        """antelopev2ç”¨ã®é¡”åˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜"""
        try:
            import cv2
            import time
            
            # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹å–å¾—
            bbox = face_info['bbox']
            x1, y1, x2, y2 = bbox
            
            # ãƒãƒ¼ã‚¸ãƒ³ã‚’è¿½åŠ 
            margin = 0.2
            width = x2 - x1
            height = y2 - y1
            x1 = max(0, int(x1 - width * margin))
            y1 = max(0, int(y1 - height * margin))
            x2 = min(image.shape[1], int(x2 + width * margin))
            y2 = min(image.shape[0], int(y2 + height * margin))
            
            # é¡”é ˜åŸŸã‚’åˆ‡ã‚Šå‡ºã—
            face_crop = image[y1:y2, x1:x2]
            
            # ä¿å­˜
            crop_dir = "static/face_crops"
            os.makedirs(crop_dir, exist_ok=True)
            
            timestamp = int(time.time() * 1000)
            base_name = os.path.splitext(os.path.basename(original_filename))[0]
            crop_filename = f"{crop_dir}/crop_{base_name}_{timestamp}.jpg"
            
            cv2.imwrite(crop_filename, face_crop)
            print(f"ğŸ’¾ Antelopev2é¡”åˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜: {crop_filename}")
            
        except Exception as e:
            print(f"âš ï¸ åˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
def get_embedding_single(filename, use_detection=True):
    """å˜ä¸€ç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—"""
    global insight_face
    return insight_face.get_embedding(filename, save_crop=False)

def get_embedding_batch(image_paths, use_detection=True):
    """ãƒãƒƒãƒå‡¦ç†ã§è¤‡æ•°ç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—"""
    global insight_face
    return insight_face.get_embeddings_batch(image_paths, save_crop=False)

def detect_and_align_face(image_path, save_crop=False):
    """ãƒ†ã‚¹ãƒˆç”¨äº’æ›é–¢æ•°ï¼šé¡”æ¤œå‡ºã¨åˆ‡ã‚Šå‡ºã—"""
    global insight_face
    if not insight_face.available:
        return None
    
    try:
        import cv2
        image = cv2.imread(image_path)
        if image is None:
            return None
        
        # BGR -> RGBå¤‰æ›
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # é¡”åˆ†æå®Ÿè¡Œï¼ˆæ¤œå‡º+èªè­˜ï¼‰
        faces = insight_face.face_app.get(rgb_image)
        
        if len(faces) == 0:
            return None
        
        # æœ€ã‚‚å¤§ãã„é¡”ã‚’é¸æŠ
        best_face = max(faces, key=lambda x: (x.bbox[2] - x.bbox[0]) * (x.bbox[3] - x.bbox[1]))
        
        # åˆ‡ã‚Šå‡ºã—ç”»åƒä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if save_crop:
            insight_face._save_face_crop(image, best_face, image_path)
        
        # é¡”é ˜åŸŸã®ã‚µã‚¤ã‚ºã‚’è¿”ã™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        bbox = best_face.bbox.astype(int)
        face_width = bbox[2] - bbox[0]
        face_height = bbox[3] - bbox[1]
        
        # ãƒ€ãƒŸãƒ¼ã®å‡ºåŠ›é…åˆ—ã‚’è¿”ã™ï¼ˆå…ƒã®é–¢æ•°ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
        return np.zeros((face_height, face_width, 3), dtype=np.uint8)
        
    except Exception as e:
        print(f"âŒ é¡”æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None

from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(
    title="ArcFace Face Recognition API",
    description="é«˜åº¦æœ€é©åŒ–ã•ã‚ŒãŸé¡”èªè¨¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ",
    version="2.0.0"
)

# å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãŸã‚ã®ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢è¨­å®š
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response

class LargeFileMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å ´åˆã®ãƒ­ã‚°å‡ºåŠ›
        if request.url.path == "/compare_folder":
            content_length = request.headers.get("content-length")
            if content_length:
                size_mb = int(content_length) / (1024 * 1024)
                if size_mb > 100:  # 100MBä»¥ä¸Š
                    print(f"ğŸ”¥ å¤§å®¹é‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ¤œå‡º: {size_mb:.1f}MB")
        
        try:
            response = await call_next(request)
            return response
        except Exception as e:
            print(f"âŒ ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã‚¨ãƒ©ãƒ¼: {e}")
            from fastapi import HTTPException
            raise HTTPException(status_code=500, detail=f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")

app.add_middleware(LargeFileMiddleware)

# å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãŸã‚ã®CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸ã®ãƒ«ãƒ¼ãƒˆè¿½åŠ 
@app.get("/benchmark_test.html", response_class=HTMLResponse)
def benchmark_test():
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‚’é…ä¿¡"""
    try:
        with open("benchmark_test.html", "r", encoding="utf-8") as f:
            content = f.read()
        return HTMLResponse(content)
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

# InsightFaceçµ±åˆãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ– - antelopev2ã‚’ä½¿ç”¨
insight_face = InsightFaceRecognition(det_size=(640, 640), model_name='antelopev2')

print("ğŸ”¥ InsightFace Antelopev2 çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¾ã™")

def get_face_embedding(image_path, save_crop=False):
    """InsightFaceã‚’ä½¿ç”¨ã—ãŸé¡”æ¤œå‡ºã¨åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡º"""
    return insight_face.get_embedding(image_path, save_crop=save_crop)

def get_embeddings_batch(file_paths, save_crop=False):
    """Antelopev2ã‚’ä½¿ç”¨ã—ãŸãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡º"""
    embeddings = []
    valid_indices = []
    
    print(f"ğŸš€ Antelopev2ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {len(file_paths)}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    for idx, file_path in enumerate(file_paths):
        try:
            embedding = insight_face.get_embedding(file_path, save_crop=save_crop)
            if embedding is not None:
                embeddings.append(embedding)
                valid_indices.append(idx)
            
            # é€²æ—è¡¨ç¤º
            if (idx + 1) % 50 == 0 or idx == len(file_paths) - 1:
                progress = (idx + 1) / len(file_paths) * 100
                print(f"ğŸ“ˆ å‡¦ç†é€²æ—: {idx + 1}/{len(file_paths)} ({progress:.1f}%)")
                
        except Exception as e:
            print(f"âŒ ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ [{idx}]: {e}")
            continue
    
    print(f"âœ… Antelopev2ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(embeddings)}å€‹ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ")
    return embeddings, valid_indices

def cosine_similarity(a, b):
    """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—"""
    return float(np.dot(a, b))

# å¤ã„é–¢æ•°ç¾¤ã‚’å‰Šé™¤æ¸ˆã¿ - InsightFaceã‚¯ãƒ©ã‚¹ã§çµ±åˆ

def adaptive_threshold(cosine_sim, euclidean_dist, base_threshold=0.45):
    """é©å¿œçš„é–¾å€¤èª¿æ•´"""
    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãŒé«˜ã„å ´åˆã¯é–¾å€¤ã‚’ä¸‹ã’ã‚‹
    if cosine_sim > 0.8:
        return base_threshold * 0.9
    elif cosine_sim > 0.6:
        return base_threshold * 0.95
    else:
        return base_threshold

def ensemble_verification(embeddings1, embeddings2):
    """è¤‡æ•°ã®æ‰‹æ³•ã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œè¨¼"""
    results = {}
    
    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
    cosine_sim = cosine_similarity(embeddings1, embeddings2)
    
    # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢
    euclidean_dist = float(np.linalg.norm(embeddings1 - embeddings2))
    
    # L1è·é›¢ï¼ˆãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ï¼‰
    l1_dist = float(np.sum(np.abs(embeddings1 - embeddings2)))
    
    # æ­£è¦åŒ–ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢
    norm_euclidean = euclidean_dist / (np.linalg.norm(embeddings1) + np.linalg.norm(embeddings2))
    
    # é©å¿œçš„é–¾å€¤
    adaptive_thresh = adaptive_threshold(cosine_sim, euclidean_dist)
    
    results = {
        'cosine_similarity': cosine_sim,
        'euclidean_distance': euclidean_dist,
        'l1_distance': l1_dist,
        'normalized_euclidean': norm_euclidean,
        'adaptive_threshold': adaptive_thresh,
        'is_same_adaptive': cosine_sim > adaptive_thresh,
        'confidence_score': min(cosine_sim * 1.2, 1.0)  # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    }
    
    return results

def compare_faces_insightface(file_path1, file_path2):
    """InsightFaceãƒ¢ãƒ‡ãƒ«ã§2ã¤ã®é¡”ã‚’æ¯”è¼ƒ"""
    start_time = time.time()
    
    # å„ç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    embedding1 = insight_face.get_embedding(file_path1, save_crop=False)
    embedding2 = insight_face.get_embedding(file_path2, save_crop=False)
    
    if embedding1 is not None and embedding2 is not None:
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œè¨¼
        ensemble_result = ensemble_verification(embedding1, embedding2)
        
        processing_time = (time.time() - start_time) * 1000  # ms
        
        return {
            'model_info': {
                'name': f'InsightFace {insight_face.model_name}',
                'description': 'InsightFaceçµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆé¡”æ¤œå‡º+èªè­˜ï¼‰',
                'embedding_size': 512
            },
            'ensemble_result': ensemble_result,
            'processing_time': processing_time,
            'error': None
        }
    else:
        return {
            'model_info': {
                'name': f'InsightFace {insight_face.model_name}',
                'description': 'InsightFaceçµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆé¡”æ¤œå‡º+èªè­˜ï¼‰',
                'embedding_size': 512
            },
            'ensemble_result': None,
            'processing_time': 0,
            'error': 'é¡”æ¤œå‡ºã¾ãŸã¯åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡ºã«å¤±æ•—'
        }


def save_temp_image(file):
    file.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
    try:
        # PILã§ç”»åƒã‚’é–‹ã„ã¦å½¢å¼ã‚’ç¢ºèªãƒ»å¤‰æ›
        img = Image.open(file)
        
        # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å½¢å¼ã‚’ç¢ºèª
        original_format = img.format
        print(f"å…ƒã®ç”»åƒå½¢å¼: {original_format}")
        
        # é€æ˜åº¦ãƒãƒ£ãƒ³ãƒãƒ«ãŒã‚ã‚‹å ´åˆã¯èƒŒæ™¯ã‚’ç™½ã«è¨­å®šã—ã¦RGBã«å¤‰æ›
        if img.mode in ('RGBA', 'LA', 'P'):
            # é€æ˜åº¦ã‚’æŒã¤ç”»åƒã®å ´åˆã€ç™½èƒŒæ™¯ã§RGBã«å¤‰æ›
            background = Image.new('RGB', img.size, (255, 255, 255))
            if img.mode == 'P':
                img = img.convert('RGBA')
            background.paste(img, mask=img.split()[-1] if img.mode in ('RGBA', 'LA') else None)
            img = background
        else:
            img = img.convert('RGB')  # RGBã«å¤‰æ›ï¼ˆã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ãªã©ã‚‚çµ±ä¸€ï¼‰
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp:
            img.save(tmp.name, 'JPEG', quality=95)
            print(f"ç”»åƒã‚’å¤‰æ›ã—ã¦ä¿å­˜: {original_format} -> JPEG")
            return tmp.name
            
    except Exception as e:
        print(f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã®æ–¹æ³•ã§ãƒã‚¤ãƒŠãƒªä¿å­˜
        try:
            file.seek(0)
            with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp:
                tmp.write(file.read())
                print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒã‚¤ãƒŠãƒªä¿å­˜ã‚’ä½¿ç”¨")
                return tmp.name
        except Exception as fallback_error:
            print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã§ã‚‚ã‚¨ãƒ©ãƒ¼: {str(fallback_error)}")
            raise fallback_error

# DeepFace function removed - using InsightFace only

@app.get("/", response_class=HTMLResponse)
def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request, "result": None})

@app.post("/verify", response_class=HTMLResponse)
def verify(request: Request, file1: UploadFile = File(...), file2: UploadFile = File(...)):
    # ç”»åƒä¿å­˜
    os.makedirs("static/uploads", exist_ok=True)
    filename1 = f"static/uploads/{uuid.uuid4().hex}_{file1.filename}"
    filename2 = f"static/uploads/{uuid.uuid4().hex}_{file2.filename}"
    with open(filename1, "wb") as buffer1:
        shutil.copyfileobj(file1.file, buffer1)
    with open(filename2, "wb") as buffer2:
        shutil.copyfileobj(file2.file, buffer2)

    # InsightFaceé¡”èªè­˜å‡¦ç†
    insightface_comparison = compare_faces_insightface(filename1, filename2)
    
    # InsightFaceåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—
    emb1_insightface = insight_face.get_embedding(filename1, save_crop=False)
    emb2_insightface = insight_face.get_embedding(filename2, save_crop=False)
    
    if emb1_insightface is not None and emb2_insightface is not None:
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        similarity_insightface = cosine_similarity(emb1_insightface, emb2_insightface)
        is_same_insightface = similarity_insightface > 0.6
        confidence_score = similarity_insightface
        processing_time = 0.0  # ç°¡å˜åŒ–ã®ãŸã‚
    else:
        similarity_insightface = 0.0
        is_same_insightface = False
        confidence_score = 0.0
        processing_time = 0.0
    
    # InsightFaceåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®è©³ç´°æƒ…å ±
    insightface_embedding_info = {
        'emb1': emb1_insightface.tolist()[:20] if emb1_insightface is not None else [],
        'emb2': emb2_insightface.tolist()[:20] if emb2_insightface is not None else [],
        'embedding_dims': 512,
        'emb1_norm': float(np.linalg.norm(emb1_insightface)) if emb1_insightface is not None else 0.0,
        'emb2_norm': float(np.linalg.norm(emb2_insightface)) if emb2_insightface is not None else 0.0
    }
    
    result = {
        "insightface": {
            "similarity": f"{similarity_insightface:.4f}",
            "is_same": is_same_insightface,
            "threshold": "0.6",
            "confidence_score": f"{confidence_score:.4f}",
            "embeddings": insightface_embedding_info,
            "processing_time": f"{processing_time:.1f}ms"
        },
        "img1_path": "/" + filename1,
        "img2_path": "/" + filename2,
        "insightface_comparison": insightface_comparison,
        "model_info": {
            "insightface": "buffalo_l",
            "description": "InsightFace Buffalo_l face recognition model"
        }
    }
    return templates.TemplateResponse("index.html", {"request": request, "result": result})

@app.post("/compare_folder_stream")
async def compare_folder_stream(
    query_image: UploadFile = File(...),
    folder_images: List[UploadFile] = File(...)
):
    """1å¯¾Næ¯”è¼ƒã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—æ›´æ–°"""
    async def generate_stream():
        try:
            # åˆæœŸåŒ–ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            yield f"data: {json.dumps({'type': 'init', 'total': len(folder_images)}, ensure_ascii=False)}\n\n"
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†å®Ÿè£…ï¼ˆç°¡ç•¥ç‰ˆï¼‰
            for i, file in enumerate(folder_images):
                progress = {
                    'type': 'progress',
                    'current': i + 1,
                    'total': len(folder_images),
                    'percentage': ((i + 1) / len(folder_images)) * 100
                }
                yield f"data: {json.dumps(progress, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.01)  # éåŒæœŸå‡¦ç†ã®ãŸã‚
            
            # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            yield f"data: {json.dumps({'type': 'complete'}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            error_msg = {'type': 'error', 'message': str(e)}
            yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache"}
    )

@app.post("/compare_folder_unlimited")
async def compare_folder_unlimited(request: Request):
    """ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™ã‚’å®Œå…¨ã«å›é¿ã™ã‚‹å°‚ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆ3520ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰"""
    try:
        print("ğŸš€ åˆ¶é™è§£é™¤å°‚ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆèµ·å‹•")
        
        # ç”Ÿã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã‚’ç›´æ¥å‡¦ç†
        content_type = request.headers.get('content-type', '')
        print(f"ğŸ“‹ Content-Type: {content_type}")
        
        if not content_type.startswith('multipart/form-data'):
            raise HTTPException(status_code=400, detail="multipart/form-dataãŒå¿…è¦ã§ã™")
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆè§£æï¼ˆåˆ¶é™ãªã—ï¼‰
        from starlette.datastructures import FormData, UploadFile
        import email
        from email.message import EmailMessage
        import io
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã‚’å–å¾—
        body = await request.body()
        print(f"ğŸ“¦ å—ä¿¡ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(body) / 1024 / 1024:.1f}MB")
        
        # ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆå¢ƒç•Œã‚’å–å¾—
        boundary = None
        for param in content_type.split(';'):
            if 'boundary=' in param:
                boundary = param.split('boundary=')[1].strip()
                break
        
        if not boundary:
            raise HTTPException(status_code=400, detail="ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆå¢ƒç•ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        print(f"ğŸ” å¢ƒç•Œæ–‡å­—åˆ—: {boundary[:20]}...")
        
        # ç°¡æ˜“ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆè§£æ
        parts = body.split(f'--{boundary}'.encode())
        print(f"ğŸ“ æ¤œå‡ºã•ã‚ŒãŸãƒ‘ãƒ¼ãƒˆæ•°: {len(parts)}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå¤šã„å ´åˆã¯å°‚ç”¨å‡¦ç†ã«è»¢é€
        if len(parts) > 1000:
            print(f"âœ… å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: {len(parts)}ãƒ‘ãƒ¼ãƒˆæ¤œå‡º")
            # å®Ÿéš›ã®å‡¦ç†ã‚’compare_folder_internalã«å§”è­²
            return JSONResponse({
                "message": f"åˆ¶é™è§£é™¤ãƒ¢ãƒ¼ãƒ‰ã§{len(parts)}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ä¿¡ã—ã¾ã—ãŸ",
                "file_count": len(parts),
                "status": "ready_for_processing"
            })
        else:
            # é€šå¸¸å‡¦ç†
            return JSONResponse({
                "message": f"é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§{len(parts)}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ä¿¡ã—ã¾ã—ãŸ",
                "file_count": len(parts)
            })
            
    except Exception as e:
        print(f"âŒ åˆ¶é™è§£é™¤ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")

@app.post("/compare_folder_large")
async def compare_folder_large(
    query_image: UploadFile = File(...),
    folder_images: List[UploadFile] = File(...),
    chunk_size: int = Form(1000)  # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’ãƒ•ã‚©ãƒ¼ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æŒ‡å®šï¼ˆåˆ¶é™è§£é™¤ï¼‰
):
    """å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å°‚ç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å¯¾å¿œ"""
    print(f"ğŸš€ å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {len(folder_images)}ãƒ•ã‚¡ã‚¤ãƒ«ã€ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º={chunk_size}")
    
    # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿å‡¦ç†ã—ã¦ã€æ®‹ã‚Šã¯åˆ¥é€”å‡¦ç†ã™ã‚‹ãŸã‚ã®è¨­è¨ˆ
    if len(folder_images) > chunk_size:
        # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã®ã¿å‡¦ç†
        first_chunk = folder_images[:chunk_size]
        print(f"ğŸ“¦ ç¬¬1ãƒãƒ£ãƒ³ã‚¯å‡¦ç†: {len(first_chunk)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        # é€šå¸¸ã®å‡¦ç†é–¢æ•°ã‚’å‘¼ã³å‡ºã—
        return await compare_folder_internal(query_image, first_chunk, is_chunk=True)
    else:
        return await compare_folder_internal(query_image, folder_images)

async def compare_folder_internal(
    query_image: UploadFile,
    folder_images: List[UploadFile],
    is_chunk: bool = False
):
    """å†…éƒ¨å‡¦ç†é–¢æ•° - å®Ÿéš›ã®æ¯”è¼ƒå‡¦ç†ã‚’å®Ÿè¡Œ"""
    start_time = time.time()
    
    # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
    print(f"ğŸ” å†…éƒ¨å‡¦ç†é–‹å§‹: query_image={query_image.filename if query_image else 'None'}")
    print(f"ğŸ” ãƒ•ã‚©ãƒ«ãƒ€ç”»åƒæ•°: {len(folder_images) if folder_images else 0}")
    print(f"ğŸ” ãƒãƒ£ãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰: {is_chunk}")
    
    try:
        # æ—¢å­˜ã®å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚³ãƒ”ãƒ¼ï¼‰
        return await _process_folder_comparison(query_image, folder_images, start_time, is_chunk)
    except Exception as e:
        import traceback
        error_message = f'å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        error_traceback = traceback.format_exc()
        print(f"âŒ å†…éƒ¨å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_message}")
        print(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼è©³ç´°:\n{error_traceback}")
        
        return JSONResponse(
            status_code=500,
            content={
                'error': error_message,
                'error_type': type(e).__name__,
                'query_image': query_image.filename if query_image else 'Unknown',
                'total_files': len(folder_images) if folder_images else 0,
                'processing_status': 'failed',
                'is_chunk': is_chunk
            }
        )

async def _process_folder_comparison(query_image, folder_images, start_time, is_chunk=False):
    """ãƒ•ã‚©ãƒ«ãƒ€æ¯”è¼ƒã®å®Ÿéš›ã®å‡¦ç†"""
    
    # åŸºæœ¬çš„ãªå…¥åŠ›æ¤œè¨¼
    if not query_image:
        return JSONResponse(
            status_code=400,
            content={'error': 'ã‚¯ã‚¨ãƒªç”»åƒãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“', 'processing_status': 'failed'}
        )
    
    if not folder_images or len(folder_images) == 0:
        return JSONResponse(
            status_code=400,
            content={'error': 'ãƒ•ã‚©ãƒ«ãƒ€ç”»åƒãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“', 'processing_status': 'failed'}
        )
    
    # å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãƒ­ã‚°ã¨æœ€é©åŒ–è¨­å®š
    total_files = len(folder_images)
    print(f"ğŸ”¥ 1å¯¾Næ¤œç´¢é–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†äºˆå®š")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèªï¼ˆ3520ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰
    cpu_count = multiprocessing.cpu_count()
    if PSUTIL_AVAILABLE:
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        available_gb = memory.available / (1024**3)
        used_pct = memory.percent
        print(f"ğŸ–¥ï¸  ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹: CPU {cpu_count}ã‚³ã‚¢, ãƒ¡ãƒ¢ãƒª {memory_gb:.1f}GB (ä½¿ç”¨ç‡{used_pct:.1f}%, åˆ©ç”¨å¯èƒ½{available_gb:.1f}GB)")
        
        # ãƒ¡ãƒ¢ãƒªä¸è¶³è­¦å‘Š
        if used_pct > 80:
            print(f"âš ï¸  ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„ã§ã™ ({used_pct:.1f}%) - å‡¦ç†ã‚’è»½é‡åŒ–ã—ã¾ã™")
        if available_gb < 2:
            print(f"âš ï¸  åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªãŒå°‘ãªã„ã§ã™ ({available_gb:.1f}GB) - ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›ã—ã¾ã™")
    else:
        print(f"ğŸ–¥ï¸  ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹: CPU {cpu_count}ã‚³ã‚¢, ãƒ¡ãƒ¢ãƒªæƒ…å ±ä¸æ˜")
    
    # æœ€é©åŒ–ã•ã‚ŒãŸå‡¦ç†è¨­å®š
    print(f"ğŸ“‹ æœ€é©åŒ–å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    use_multiprocessing = False
    batch_size = min(32, max(1, total_files // 4))  # æœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨ˆç®—
    max_workers = 1
    memory_cleanup_interval = 100
    chunk_processing = False
    
    print(f"æœ€é©åŒ–è¨­å®š: ãƒãƒƒãƒã‚µã‚¤ã‚º={batch_size}, ä¸¦åˆ—æ•°={max_workers}, ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚·ãƒ³ã‚°={use_multiprocessing}")
    if 'chunk_processing' in locals() and chunk_processing:
        print(f"æ®µéšçš„å‡¦ç†: ãƒãƒƒãƒã‚µã‚¤ã‚º={batch_size}ã§åˆ†å‰²å‡¦ç†")
    
    # ã‚¯ã‚¨ãƒªç”»åƒã‚’ä¿å­˜
    os.makedirs("static/temp", exist_ok=True)
    query_filename = f"static/temp/query_{uuid.uuid4().hex}_{query_image.filename}"
    with open(query_filename, "wb") as buffer:
        shutil.copyfileobj(query_image.file, buffer)
    
    print(f"ã‚¯ã‚¨ãƒªç”»åƒä¿å­˜å®Œäº†: {query_filename}")
    
    # Buffalo_lãƒ¢ãƒ‡ãƒ«ã§ã‚¯ã‚¨ãƒªç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    query_embedding = insight_face.get_embedding(query_filename, save_crop=False)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å‡¦ç†ã‚’å®Ÿè¡Œ
    file_info_list = await _save_files_individually(folder_images)
    
    print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {len(file_info_list)}ä»¶")
    
    # ä¿å­˜ã‚¨ãƒ©ãƒ¼ãŒãªã„ã‹ãƒã‚§ãƒƒã‚¯
    failed_saves = [f for f in file_info_list if f.get('error')]
    if failed_saves:
        print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {len(failed_saves)}ä»¶")
        for failed in failed_saves[:10]:  # æœ€åˆã®10ä»¶ã¾ã§è¡¨ç¤ºï¼ˆå…¨ä»¶ã¯å†—é•·ãªãŸã‚ï¼‰
            print(f"  - {failed['original_name']}: {failed.get('error', 'Unknown error')}")
    
    # æˆåŠŸã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å‡¦ç†å¯¾è±¡ã¨ã™ã‚‹
    valid_file_info_list = [f for f in file_info_list if not f.get('error') and f.get('filename')]
    print(f"ğŸ“Š å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(valid_file_info_list)}ä»¶")
    
    if not valid_file_info_list:
        return JSONResponse(
            status_code=400,
            content={
                'error': 'ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ',
                'failed_files': len(failed_saves),
                'processing_status': 'failed'
            }
        )
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªé †æ¬¡å‡¦ç†ã‚’å®Ÿè¡Œ
    print(f"ğŸ”„ é †æ¬¡å‡¦ç†é–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    results = await _execute_comparison_buffalo(query_embedding, valid_file_info_list, batch_size, start_time)
    
    # çµæœã®æ•´ç†ã¨è¿”å´
    return _format_comparison_results(results, query_image, total_files, valid_file_info_list, start_time, is_chunk)

async def _save_files_individually(folder_images):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å‡¦ç†ï¼ˆæœ€é©åŒ–ãªã—ï¼‰"""
    file_info_list = []
    temp_dir = "static/temp"
    os.makedirs(temp_dir, exist_ok=True)
    
    total_files = len(folder_images)
    print(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # 1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤é †æ¬¡ä¿å­˜
    for idx, folder_image in enumerate(folder_images):
        try:
            # é€²æ—è¡¨ç¤º
            if idx % 100 == 0 or idx == total_files - 1:
                progress_pct = (idx + 1) / total_files * 100
                print(f"ğŸ“‹ ä¿å­˜ä¸­: {idx + 1}/{total_files} ({progress_pct:.1f}%)")
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            safe_name = f"file_{idx}_{uuid.uuid4().hex[:8]}.jpg"
            file_path = f"{temp_dir}/{safe_name}"
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            folder_image.file.seek(0)
            content = await folder_image.read()
            
            with open(file_path, "wb") as f:
                f.write(content)
            
            file_info_list.append({
                'filename': file_path,
                'original_name': folder_image.filename,
                'index': idx
            })
            
        except Exception as e:
            print(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼ [{idx}]: {e}")
            file_info_list.append({
                'filename': None,
                'original_name': folder_image.filename or f"image_{idx}",
                'index': idx,
                'error': str(e)
            })
    
    print(f"âœ… ä¿å­˜å®Œäº†: {len(file_info_list)}ãƒ•ã‚¡ã‚¤ãƒ«")
    return file_info_list

async def _execute_chunked_comparison(query_embeddings, valid_file_info_list, selected_models, use_multiprocessing, batch_size, max_workers, memory_cleanup_interval, start_time, query_filename, chunk_size):
    """æ®µéšçš„æ¯”è¼ƒå‡¦ç†ã®å®Ÿè¡Œï¼ˆ3530ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰"""
    all_results = []
    total_files = len(valid_file_info_list)
    processed_files = 0
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
    for chunk_idx in range(0, total_files, chunk_size):
        chunk_end = min(chunk_idx + chunk_size, total_files)
        current_chunk = valid_file_info_list[chunk_idx:chunk_end]
        chunk_number = (chunk_idx // chunk_size) + 1
        total_chunks = (total_files + chunk_size - 1) // chunk_size
        
        print(f"ğŸ“¦ ãƒãƒ£ãƒ³ã‚¯ {chunk_number}/{total_chunks} å‡¦ç†ä¸­: {len(current_chunk)}ãƒ•ã‚¡ã‚¤ãƒ« ({chunk_idx+1}-{chunk_end})")
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
        chunk_results = await _execute_comparison(query_embeddings, current_chunk, selected_models, use_multiprocessing, batch_size, max_workers, memory_cleanup_interval, start_time, query_filename)
        all_results.extend(chunk_results)
        
        processed_files += len(current_chunk)
        progress_pct = (processed_files / total_files) * 100
        print(f"âœ… ãƒãƒ£ãƒ³ã‚¯ {chunk_number} å®Œäº†: ç´¯è¨ˆ {processed_files}/{total_files} ({progress_pct:.1f}%)")
        
        # ãƒãƒ£ãƒ³ã‚¯é–“ã§ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if chunk_number < total_chunks:  # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã§ãªã„å ´åˆ
            print("ğŸ§¹ ãƒãƒ£ãƒ³ã‚¯é–“ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ")
            gc.collect()
            await asyncio.sleep(0.2)  # çŸ­ã„ä¼‘æ†©
    
    print(f"ğŸ‰ æ®µéšçš„å‡¦ç†å®Œäº†: å…¨{total_chunks}ãƒãƒ£ãƒ³ã‚¯, {total_files}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ¸ˆã¿")
    return all_results

async def _execute_comparison_buffalo(query_embedding, valid_file_info_list, batch_size, start_time):
    """Antelopev2ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒãƒƒãƒå‡¦ç†æ¯”è¼ƒ"""
    total_files = len(valid_file_info_list)
    
    print(f"ğŸš€ Antelopev2 ãƒãƒƒãƒå‡¦ç†æ¯”è¼ƒé–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # ãƒãƒƒãƒå‡¦ç†ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸€æ‹¬å–å¾—
    target_file_paths = [file_info['filename'] for file_info in valid_file_info_list]
    
    print(f"ğŸ“Š ãƒãƒƒãƒç‰¹å¾´é‡æŠ½å‡ºé–‹å§‹... (ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size})")
    target_embeddings, valid_indices = get_embedding_batch(
        target_file_paths, 
        use_detection=True
    )
    
    if target_embeddings.size == 0:
        print("âŒ ãƒãƒƒãƒç‰¹å¾´é‡æŠ½å‡ºã«å¤±æ•—")
        return []
    
    print(f"âœ… ãƒãƒƒãƒç‰¹å¾´é‡æŠ½å‡ºå®Œäº†: {len(target_embeddings)}å€‹ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«")
    
    # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    query_emb = query_embedding
    if query_emb is None:
        print("âŒ ã‚¯ã‚¨ãƒªç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    
    print(f"ğŸ”„ é¡ä¼¼åº¦è¨ˆç®—é–‹å§‹...")
    results = []
    
    # ãƒãƒƒãƒã§å–å¾—ã—ãŸåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨æ¯”è¼ƒ
    for i, (target_emb, file_idx) in enumerate(zip(target_embeddings, valid_indices)):
        try:
            file_info = valid_file_info_list[file_idx]
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ï¼ˆé«˜é€ŸåŒ–ï¼‰
            similarity_score = float(np.dot(query_emb, target_emb))
            
            # çµæœã‚’è¿½åŠ 
            results.append({
                'filename': os.path.basename(file_info['filename']),
                'original_filename': file_info['original_name'],
                'image_path': "/" + file_info['filename'],
                'best_similarity': similarity_score,
                'best_model': 'buffalo_l',
                'model_results': {
                    'buffalo_l': {
                        'model_name': 'buffalo_l',
                        'similarity': similarity_score,
                        'confidence': min(similarity_score * 1.2, 1.0),
                        'is_same': similarity_score > 0.45
                    }
                },
                'is_match': similarity_score > 0.45
            })
            
            # é€²æ—è¡¨ç¤º
            if (i + 1) % 500 == 0 or i == len(target_embeddings) - 1:
                progress = (i + 1) / len(target_embeddings) * 100
                print(f"ğŸ“ˆ é¡ä¼¼åº¦è¨ˆç®—: {i + 1}/{len(target_embeddings)} ({progress:.1f}%)")
            
        except Exception as e:
            print(f"âŒ é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼ [{i}]: {e}")
            file_info = valid_file_info_list[file_idx] if file_idx < len(valid_file_info_list) else {'filename': 'unknown', 'original_name': 'unknown'}
            results.append({
                'filename': os.path.basename(file_info.get('filename', 'unknown')),
                'original_filename': file_info.get('original_name', 'unknown'),
                'image_path': None,
                'best_similarity': 0.0,
                'best_model': 'N/A',
                'model_results': {},
                'is_match': False,
                'error': str(e)
            })
    
    # å‡¦ç†ã§ããªã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒãƒƒãƒå‡¦ç†ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’è¿½åŠ 
    processed_indices = set(valid_indices)
    for idx, file_info in enumerate(valid_file_info_list):
        if idx not in processed_indices:
            results.append({
                'filename': os.path.basename(file_info['filename']),
                'original_filename': file_info['original_name'],
                'image_path': "/" + file_info['filename'],
                'best_similarity': 0.0,
                'best_model': 'N/A',
                'model_results': {},
                'is_match': False,
                'error': 'ãƒãƒƒãƒå‡¦ç†ã§ã‚¹ã‚­ãƒƒãƒ—'
            })
    
    print(f"âœ… Antelopev2 ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(results)}ä»¶ã®çµæœ")
    
    # é¡ä¼¼åº¦ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x['best_similarity'], reverse=True)
    
    # é †ä½ã‚’è¿½åŠ 
    for idx, result in enumerate(results):
        result['rank'] = idx + 1
    
    top10_similarities = [f"{r['best_similarity']:.3f}" for r in results[:10]]
    print(f"ğŸ† çµæœã‚½ãƒ¼ãƒˆå®Œäº†: ä¸Šä½10ä»¶ã®é¡ä¼¼åº¦ {top10_similarities}")
    
    processing_time = time.time() - start_time
    print(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’ ({len(results)/processing_time:.1f}ãƒ•ã‚¡ã‚¤ãƒ«/ç§’)")
    
    return results


async def _execute_comparison_no_batch(query_embedding, valid_file_info_list, start_time):
    """ãƒãƒƒãƒå‡¦ç†ãªã—ã®æ¯”è¼ƒå‡¦ç†ï¼ˆé€Ÿåº¦æ¯”è¼ƒç”¨ï¼‰"""
    total_files = len(valid_file_info_list)
    
    print(f"ğŸŒ éãƒãƒƒãƒå‡¦ç†æ¯”è¼ƒé–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤é †æ¬¡å‡¦ç†ï¼‰")
    
    # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    query_emb = query_embedding
    if query_emb is None:
        print("âŒ ã‚¯ã‚¨ãƒªç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    
    print(f"ğŸ”„ é †æ¬¡å‡¦ç†ã«ã‚ˆã‚‹é¡ä¼¼åº¦è¨ˆç®—é–‹å§‹...")
    results = []
    
    # 1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤é †æ¬¡å‡¦ç†
    for i, file_info in enumerate(valid_file_info_list):
        try:
            # 1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
            target_emb = get_embedding_single(file_info['filename'], use_detection=True)
            
            if target_emb is None:
                # å‡¦ç†å¤±æ•—ã®å ´åˆ
                results.append({
                    'filename': os.path.basename(file_info['filename']),
                    'original_filename': file_info['original_name'],
                    'image_path': "/" + file_info['filename'],
                    'best_similarity': 0.0,
                    'best_model': 'N/A',
                    'model_results': {},
                    'is_match': False,
                    'error': 'åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡ºå¤±æ•—'
                })
                continue
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
            similarity_score = float(np.dot(query_emb, target_emb))
            
            # çµæœã‚’è¿½åŠ 
            results.append({
                'filename': os.path.basename(file_info['filename']),
                'original_filename': file_info['original_name'],
                'image_path': "/" + file_info['filename'],
                'best_similarity': similarity_score,
                'best_model': 'buffalo_l',
                'model_results': {
                    'buffalo_l': {
                        'model_name': 'buffalo_l',
                        'similarity': similarity_score,
                        'confidence': min(similarity_score * 1.2, 1.0),
                        'is_same': similarity_score > 0.45
                    }
                },
                'is_match': similarity_score > 0.45
            })
            
            # é€²æ—è¡¨ç¤ºï¼ˆéãƒãƒƒãƒå‡¦ç†ã¯é…ã„ã®ã§ã‚ˆã‚Šé »ç¹ã«è¡¨ç¤ºï¼‰
            if (i + 1) % 50 == 0 or i == total_files - 1:
                progress = (i + 1) / total_files * 100
                elapsed_time = time.time() - start_time
                estimated_total = elapsed_time * total_files / (i + 1) if i > 0 else 0
                remaining_time = estimated_total - elapsed_time
                print(f"ğŸ“ˆ éãƒãƒƒãƒå‡¦ç†: {i + 1}/{total_files} ({progress:.1f}%) - æ®‹ã‚Šæ™‚é–“: {remaining_time:.1f}ç§’")
            
        except Exception as e:
            print(f"âŒ éãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ [{i}]: {e}")
            results.append({
                'filename': os.path.basename(file_info.get('filename', 'unknown')),
                'original_filename': file_info.get('original_name', 'unknown'),
                'image_path': "/" + file_info['filename'] if file_info.get('filename') else None,
                'best_similarity': 0.0,
                'best_model': 'N/A',
                'model_results': {},
                'is_match': False,
                'error': str(e)
            })
    
    print(f"âœ… éãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(results)}ä»¶ã®çµæœ")
    
    # é¡ä¼¼åº¦ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x['best_similarity'], reverse=True)
    
    # é †ä½ã‚’è¿½åŠ 
    for idx, result in enumerate(results):
        result['rank'] = idx + 1
    
    top10_similarities = [f"{r['best_similarity']:.3f}" for r in results[:10]]
    print(f"ğŸ† éãƒãƒƒãƒçµæœã‚½ãƒ¼ãƒˆå®Œäº†: ä¸Šä½10ä»¶ã®é¡ä¼¼åº¦ {top10_similarities}")
    
    processing_time = time.time() - start_time
    print(f"â±ï¸ éãƒãƒƒãƒç·å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’ ({len(results)/processing_time:.1f}ãƒ•ã‚¡ã‚¤ãƒ«/ç§’)")
    
    return results

async def _execute_comparison(query_embeddings, valid_file_info_list, selected_models, use_multiprocessing, batch_size, max_workers, memory_cleanup_interval, start_time, query_filename):
    """ãƒãƒƒãƒå‡¦ç†ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿæ¯”è¼ƒå‡¦ç†"""
    total_files = len(valid_file_info_list)
    
    print(f"ğŸš€ ãƒãƒƒãƒå‡¦ç†æ¯”è¼ƒé–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # ãƒãƒƒãƒå‡¦ç†ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¸€æ‹¬å–å¾—
    target_file_paths = [file_info['filename'] for file_info in valid_file_info_list]
    
    print(f"ğŸ“Š ãƒãƒƒãƒç‰¹å¾´é‡æŠ½å‡ºé–‹å§‹... (ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size})")
    target_embeddings, valid_indices = get_embedding_batch(
        target_file_paths, 
        use_detection=True
    )
    
    if target_embeddings.size == 0:
        print("âŒ ãƒãƒƒãƒç‰¹å¾´é‡æŠ½å‡ºã«å¤±æ•—")
        return []
    
    print(f"âœ… ãƒãƒƒãƒç‰¹å¾´é‡æŠ½å‡ºå®Œäº†: {len(target_embeddings)}å€‹ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«")
    
    # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    query_emb = query_embeddings.get('buffalo_l', {}).get('embedding')
    if query_emb is None:
        print("âŒ ã‚¯ã‚¨ãƒªç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    
    print(f"ğŸ”„ é¡ä¼¼åº¦è¨ˆç®—é–‹å§‹...")
    results = []
    
    # ãƒãƒƒãƒã§å–å¾—ã—ãŸåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨æ¯”è¼ƒ
    for i, (target_emb, file_idx) in enumerate(zip(target_embeddings, valid_indices)):
        try:
            file_info = valid_file_info_list[file_idx]
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ï¼ˆé«˜é€ŸåŒ–ï¼‰
            similarity_score = float(np.dot(query_emb, target_emb))
            
            # çµæœã‚’è¿½åŠ 
            results.append({
                'filename': os.path.basename(file_info['filename']),
                'original_filename': file_info['original_name'],
                'image_path': "/" + file_info['filename'],
                'best_similarity': similarity_score,
                'best_model': 'buffalo_l',
                'model_results': {
                    'buffalo_l': {
                        'model_name': 'buffalo_l',
                        'similarity': similarity_score,
                        'confidence': min(similarity_score * 1.2, 1.0),
                        'is_same': similarity_score > 0.45
                    }
                },
                'is_match': similarity_score > 0.45
            })
            
            # é€²æ—è¡¨ç¤º
            if (i + 1) % 500 == 0 or i == len(target_embeddings) - 1:
                progress = (i + 1) / len(target_embeddings) * 100
                print(f"ğŸ“ˆ é¡ä¼¼åº¦è¨ˆç®—: {i + 1}/{len(target_embeddings)} ({progress:.1f}%)")
            
        except Exception as e:
            print(f"âŒ é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼ [{i}]: {e}")
            file_info = valid_file_info_list[file_idx] if file_idx < len(valid_file_info_list) else {'filename': 'unknown', 'original_name': 'unknown'}
            results.append({
                'filename': os.path.basename(file_info.get('filename', 'unknown')),
                'original_filename': file_info.get('original_name', 'unknown'),
                'image_path': None,
                'best_similarity': 0.0,
                'best_model': 'N/A',
                'model_results': {},
                'is_match': False,
                'error': str(e)
            })
    
    # å‡¦ç†ã§ããªã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒãƒƒãƒå‡¦ç†ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’è¿½åŠ 
    processed_indices = set(valid_indices)
    for idx, file_info in enumerate(valid_file_info_list):
        if idx not in processed_indices:
            results.append({
                'filename': os.path.basename(file_info['filename']),
                'original_filename': file_info['original_name'],
                'image_path': "/" + file_info['filename'],
                'best_similarity': 0.0,
                'best_model': 'N/A',
                'model_results': {},
                'is_match': False,
                'error': 'ãƒãƒƒãƒå‡¦ç†ã§ã‚¹ã‚­ãƒƒãƒ—'
            })
    
    print(f"âœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(results)}ä»¶ã®çµæœ")
    
    # é¡ä¼¼åº¦ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x['best_similarity'], reverse=True)
    
    # é †ä½ã‚’è¿½åŠ 
    for idx, result in enumerate(results):
        result['rank'] = idx + 1
    
    top10_similarities = [f"{r['best_similarity']:.3f}" for r in results[:10]]
    print(f"ğŸ† çµæœã‚½ãƒ¼ãƒˆå®Œäº†: ä¸Šä½10ä»¶ã®é¡ä¼¼åº¦ {top10_similarities}")
    
    processing_time = time.time() - start_time
    print(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’ ({len(results)/processing_time:.1f}ãƒ•ã‚¡ã‚¤ãƒ«/ç§’)")
    
    return results

def _format_comparison_results(results, query_image, total_files, valid_file_info_list, start_time, is_chunk=False):
    """çµæœã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    total_processing_time = time.time() - start_time
    files_per_second = len(results) / total_processing_time if total_processing_time > 0 else 0
    
    return JSONResponse(content={
        'query_image': query_image.filename,
        'total_comparisons': total_files,
        'successful_comparisons': len(results),
        'matches_found': len([r for r in results if r['is_match']]),
        'results': results,  # å…¨ä»¶è¡¨ç¤º
        'total_results': len(results),
        'showing_top': len(results),
        'is_chunk': is_chunk,
        'processing_summary': {
            'model': 'buffalo_l',
            'model_description': 'InsightFace Buffalo_l face recognition model',
            'threshold_used': 0.45,
            'total_processing_time': total_processing_time * 1000,
            'files_per_second': files_per_second,
            'optimization_level': 'high' if total_files > 1000 else 'standard'
        }
    })

@app.post("/compare_folder_benchmark")
async def compare_folder_benchmark(
    query_image: UploadFile = File(...),
    folder_images: List[UploadFile] = File(...),
    use_batch: bool = Form(True)  # ãƒãƒƒãƒå‡¦ç†ã®æœ‰ç„¡ã‚’é¸æŠ
):
    """é€Ÿåº¦æ¯”è¼ƒç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - ãƒãƒƒãƒå‡¦ç†ã‚ã‚Š/ãªã—ã®æ€§èƒ½æ¯”è¼ƒ"""
    start_time = time.time()
    total_files = len(folder_images)
    
    print(f"ğŸ é€Ÿåº¦æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«, ãƒãƒƒãƒå‡¦ç†={'æœ‰åŠ¹' if use_batch else 'ç„¡åŠ¹'}")
    
    try:
        # ã‚¯ã‚¨ãƒªç”»åƒã‚’ä¿å­˜
        os.makedirs("static/temp", exist_ok=True)
        query_filename = f"static/temp/query_{uuid.uuid4().hex}_{query_image.filename}"
        with open(query_filename, "wb") as buffer:
            shutil.copyfileobj(query_image.file, buffer)
        
        # ã‚¯ã‚¨ãƒªç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
        query_embedding = insight_face.get_embedding(query_filename, save_crop=False)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å‡¦ç†
        file_info_list = await _save_files_individually(folder_images)
        valid_file_info_list = [f for f in file_info_list if not f.get('error') and f.get('filename')]
        
        preprocessing_time = time.time() - start_time
        comparison_start_time = time.time()
        
        if use_batch:
            # ãƒãƒƒãƒå‡¦ç†ç‰ˆ
            optimal_batch_size = min(32, max(1, len(valid_file_info_list) // 4))
            print(f"ğŸš€ ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ (æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º: {optimal_batch_size})")
            results = await _execute_comparison_buffalo(query_embedding, valid_file_info_list, optimal_batch_size, comparison_start_time)
        else:
            # éãƒãƒƒãƒå‡¦ç†ç‰ˆ
            print(f"ğŸŒ éãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ (1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤é †æ¬¡å‡¦ç†)")
            results = await _execute_comparison_no_batch(query_embedding, valid_file_info_list, comparison_start_time)
        
        comparison_time = time.time() - comparison_start_time
        total_time = time.time() - start_time
        
        # é€Ÿåº¦çµ±è¨ˆã®è¨ˆç®—
        files_per_second = len(valid_file_info_list) / comparison_time if comparison_time > 0 else 0
        
        # çµæœã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: resultså‹={type(results)}, é•·ã•={len(results) if results else 'None'}")
        if results:
            print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®çµæœ={results[0] if len(results) > 0 else 'Empty'}")
        
        # top_matchesã®ãƒ‡ãƒãƒƒã‚°
        top_matches = results[:10] if results else []
        print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: top_matcheså‹={type(top_matches)}, é•·ã•={len(top_matches)}")
        
        return JSONResponse(content={
            'benchmark_mode': 'batch' if use_batch else 'no_batch',
            'query_image': query_image.filename,
            'total_files': total_files,
            'processed_files': len(valid_file_info_list),
            'matches_found': len([r for r in results if r['is_match']]) if results else 0,
            'results': results[:100] if results else [],  # ä¸Šä½100ä»¶ã®ã¿è¡¨ç¤º
            'total_results': len(results) if results else 0,
            'performance_metrics': {
                'preprocessing_time_ms': preprocessing_time * 1000,
                'comparison_time_ms': comparison_time * 1000,
                'total_time_ms': total_time * 1000,
                'files_per_second': files_per_second,
                'avg_time_per_file_ms': (comparison_time / len(valid_file_info_list) * 1000) if valid_file_info_list else 0,
                'processing_method': 'ãƒãƒƒãƒå‡¦ç†' if use_batch else 'é †æ¬¡å‡¦ç†ï¼ˆéãƒãƒƒãƒï¼‰',
                'efficiency_score': files_per_second * len(valid_file_info_list)  # ç·åˆåŠ¹ç‡ã‚¹ã‚³ã‚¢
            },
            'top_matches': top_matches
        })
        
    except Exception as e:
        import traceback
        error_traceback = traceback.format_exc()
        print(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")
        print(f"ğŸ“‹ ã‚¨ãƒ©ãƒ¼è©³ç´°:\n{error_traceback}")
        
        return JSONResponse(
            status_code=500,
            content={
                'error': f'ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}',
                'benchmark_mode': 'batch' if use_batch else 'no_batch',
                'total_files': total_files,
                'processing_status': 'failed'
            }
        )

@app.post("/compare_folder")
async def compare_folder(
    request: Request,
    query_image: UploadFile = File(...),
    folder_images: List[UploadFile] = File(...)
):
    """1å¯¾Næ¯”è¼ƒã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - 1ã¤ã®ç”»åƒã¨ãƒ•ã‚©ãƒ«ãƒ€ã®å…¨ç”»åƒã‚’æ¯”è¼ƒï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™è§£é™¤ç‰ˆï¼‰"""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°åˆ¶é™ã®äº‹å‰ãƒã‚§ãƒƒã‚¯ã¨ãƒã‚¤ãƒ‘ã‚¹
    file_count = len(folder_images) if folder_images else 0
    print(f"ğŸ“‹ å—ä¿¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}")
    
    if file_count > 1000:
        print(f"ğŸš¨ å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {file_count}ãƒ•ã‚¡ã‚¤ãƒ« - åˆ¶é™è§£é™¤ãƒ¢ãƒ¼ãƒ‰ã§å‡¦ç†")
        
        # å®Ÿè¡Œæ™‚åˆ¶é™ã®å¼·åˆ¶è§£é™¤
        try:
            import starlette.formparsers as fp
            if hasattr(fp, 'MAX_FORM_FILES'):
                original = fp.MAX_FORM_FILES
                fp.MAX_FORM_FILES = 100000  # æ¥µé™ã¾ã§å¢—åŠ 
                print(f"ğŸ”§ å®Ÿè¡Œæ™‚åˆ¶é™å¼·åŒ–: MAX_FORM_FILES {original} â†’ 100000")
                
            # ãã®ä»–ã®åˆ¶é™ã‚‚ç·©å’Œ
            if hasattr(fp, 'MAX_FORM_PART_SIZE'):
                fp.MAX_FORM_PART_SIZE = 200 * 1024 * 1024  # 200MB
                print(f"ğŸ”§ ãƒ‘ãƒ¼ãƒˆã‚µã‚¤ã‚ºåˆ¶é™å¼·åŒ–: 200MB")
                
        except Exception as e:
            print(f"âš ï¸ å®Ÿè¡Œæ™‚åˆ¶é™è§£é™¤ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"ğŸ¯ åˆ¶é™ç¢ºèªå®Œäº† - å‡¦ç†é–‹å§‹å¯èƒ½")
    start_time = time.time()
    
    try:
        # è©³ç´°ãƒªã‚¯ã‚¨ã‚¹ãƒˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        content_length = request.headers.get("content-length", "ä¸æ˜")
        content_type = request.headers.get("content-type", "ä¸æ˜")
        user_agent = request.headers.get("user-agent", "ä¸æ˜")
        
        print(f"ğŸŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆè©³ç´°:")
        print(f"   ğŸ“ Content-Length: {content_length}")
        print(f"   ğŸ“‹ Content-Type: {content_type}")
        print(f"   ğŸ–¥ï¸  User-Agent: {user_agent}")
        print(f"   ğŸ“ ã‚¯ã‚¨ãƒªç”»åƒ: {query_image.filename if query_image else 'None'}")
        print(f"   ğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€ç”»åƒæ•°: {len(folder_images) if folder_images else 0}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®æ¨å®š
        if folder_images and len(folder_images) > 0:
            sample_size = 0
            sample_count = min(5, len(folder_images))
            for i in range(sample_count):
                try:
                    content = await folder_images[i].read()
                    size = len(content)
                    sample_size += size
                    await folder_images[i].seek(0)  # ãƒã‚¤ãƒ³ã‚¿ã‚’æˆ»ã™
                    print(f"   ğŸ“ ã‚µãƒ³ãƒ—ãƒ«{i+1}: {folder_images[i].filename} ({size:,}ãƒã‚¤ãƒˆ)")
                except Exception as e:
                    print(f"   âŒ ã‚µãƒ³ãƒ—ãƒ«{i+1}èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
            
            if sample_count > 0:
                avg_size = sample_size / sample_count
                estimated_total = avg_size * len(folder_images)
                print(f"   ğŸ’¾ æ¨å®šç·ã‚µã‚¤ã‚º: {estimated_total / (1024*1024):.1f}MB")
                
                # å¤§é‡ãƒ‡ãƒ¼ã‚¿è­¦å‘Š
                if estimated_total > 500 * 1024 * 1024:  # 500MBä»¥ä¸Š
                    print(f"âš ï¸  å¤§é‡ãƒ‡ãƒ¼ã‚¿è­¦å‘Š: æ¨å®š{estimated_total / (1024*1024):.1f}MB")
        
    except Exception as e:
        print(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆè§£æã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
    print(f"ğŸ” ãƒªã‚¯ã‚¨ã‚¹ãƒˆå—ä¿¡: query_image={query_image.filename if query_image else 'None'}")
    print(f"ğŸ” ãƒ•ã‚©ãƒ«ãƒ€ç”»åƒæ•°: {len(folder_images) if folder_images else 0}")
    
    # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚·ãƒ³ãƒ—ãƒ«ãªé †æ¬¡å‡¦ç†ã§å®Ÿè¡Œ
    print(f"ğŸ“‹ ã‚·ãƒ³ãƒ—ãƒ«å‡¦ç†å®Ÿè¡Œ: {len(folder_images)}ãƒ•ã‚¡ã‚¤ãƒ«")
    return await compare_folder_internal(query_image, folder_images)
